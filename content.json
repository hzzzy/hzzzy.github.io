[{"title":"nodejs waterfall的使用","date":"2018-10-11T09:15:11.000Z","path":"2018/10/11/waterfall-use/","text":"waterfall(tasks, [callback]) （多个函数依次执行，且前一个的输出为后一个的输入）按顺序依次执行多个函数。每一个函数产生的值，都将传给下一个函数。如果中途出错，后面的函数将不会被执行。错误信息以及之前产生的结果，将传给waterfall最终的callback。 对于学过了js回调机制的小伙伴，waterfall是比较容易理解的，个人的理解就是，waterfall中传入的函数数组tasks中，后一个函数为前一个函数的回调，使用cb(null,args)，这样的形式调用下一个函数，如果出现异常，则直接使用cb(new Error(“错误的信息”))这样的方式来捕捉异常，并调用最终的回调函数来处理，在这种情况下，出现异常的函数后面那些函数，将不再继续执行，测试代码如下： 12345678910111213141516171819202122232425262728293031323334var async = require(&apos;async&apos;);var a = 10;async.waterfall([ function(cb) &#123; console.log(&quot;getb&quot;) setTimeout(function() &#123; if (a == 0) &#123; cb(new Error(&quot;a不能为0&quot;)); &#125; else &#123; var b = 1 / a; cb(null, b); //在这里通过回调函数把b传给下一个函数，记得一定要加上null，才能调用数组中得下一个函数，否则，会直接调用最终的回调函数，然后结束函数，则后面的函数将不再执行 //如果这里写成cb(b); //结果会变成： /** *getb *0.1 **/ &#125; &#125;, 1000); &#125;, function(b, cb) &#123; setTimeout(function() &#123; console.log(&quot;getc&quot;) var c = b + 1; cb(null,c); &#125;, 1000); &#125;], function(err, result) &#123; if (err) &#123; console.log(err); &#125; else &#123; console.log(&apos;c:&apos; + result) &#125;&#125;); 结果：123456789当a = 0时，会直接抛出错误，输出如下：getbError: a不能为0先执行了第一个函数，在第一个函数中抛出异常之后，直接执行最终的回调函数，并没有接着执行第二个函数。a = 10 时，输出如下：getbgetc1.1先执行了第一个函数，然后把第一个函数算出的b传给了第二个函数，再次算出第二个函数中得C，传给最终的结果result。","tags":[{"name":"nodejs","slug":"nodejs","permalink":"www.hzzzy.top/tags/nodejs/"}]},{"title":"HBase1.1.2安装","date":"2018-05-18T08:09:59.000Z","path":"2018/05/18/hbase-install/","text":"一、说明HBase搭建的前提是Hadoop和Zookeeper均已搭建完毕，并确保各节点时钟一致，避免因时间不一致而导致HMaster启动失败。一个分布式运行的HBase依赖一个Zookeeper集群。所有的节点和客户端都必须能够访问Zookeeper。默认的情况下HBase会管理一个Zookeeper集群。这个集群会随着HBase的启动而启动。当然，我们也可以自己管理一个Zookeeper集群，但需要配置HBase。需要修改conf/hbase-env.sh里面的HBASE_MANAGES_ZK 来切换。这个值默认是true的，作用是让HBase启动的时候同时也启动Zookeeper。让HBase使用一个现有的不被HBase托管的Zookeeper集群，需要设置 conf/hbase-env.sh文件中的HBASE_MANAGES_ZK 属性为 false，并在文件hbase-site.xml添加hbase.zookeeper.quorum的值。 二、安装以hadoop用户将文件 hbase-1.1.2-bin.tar.gz 拷贝到目录/opt：解压安装：tar zxvf hbase-1.1.2-bin.tar.gz安装完毕 三、配置HBase1、修改环境变量1234567sudo vi /etc/profile #添加或修改 export JAVA_HOME=/opt/hadoop2/jdk1.7.0_79export HADOOP_HOME=/opt/hadoop2/hadoop-2.7.1export HBASE_HOME=/opt/hbase-1.1.2export ZOOKEEPER_HOME=/opt/zookeeper-3.4.6export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$HADOOP_HOME/bin:$HBASE_HOME/bin:$ZOOKEEPER_HOME/bin 验证输入命令：hbase version12345[hadoop@ycf-113 hbase-1.1.2]$ hbase version2015-12-18 11:09:04,864 INFO [main] util.VersionInfo: HBase 1.1.22015-12-18 11:09:04,864 INFO [main] util.VersionInfo: Source code repository git://hw11397.local/Volumes/hbase-1.1.2RC2/hbase revision=cc2b70cf03e3378800661ec5cab11eb43fafe0fc2015-12-18 11:09:04,864 INFO [main] util.VersionInfo: Compiled by ndimiduk on Wed Aug 26 20:11:27 PDT 20152015-12-18 11:09:04,864 INFO [main] util.VersionInfo: From source with checksum 73da41f3d1b867b7aba6166c77fafc17 2、配置4个文件1）hbase-env.sh1234567vi /opt/hbase-1.1.2/conf/hbase-env.shexport JAVA_HOME=/opt/hadoop2/jdk1.7.0_79export HBASE_CLASSPATH=/opt/hadoop2/hadoop-2.7.1/etc/hadoopexport HBASE_HEAPSIZE=1Gexport HBASE_LOG_DIR=$&#123;HBASE_HOME&#125;/logsexport HBASE_PID_DIR=$&#123;HBASE_HOME&#125;/pidsexport HBASE_MANAGES_ZK=false 2）hbase-site.xml12345678910111213141516171819202122232425262728293031vi /opt/hbase-1.1.2/conf/hbase-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;hdfs://ycf-113:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ycf-113:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;ycf-114&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hbase-1.1.2/hbase-tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/hbase-1.1.2/zookeeper-tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 其中，hbase.master是指定运行HMaster的服务器及端口号；hbase.rootdir指定HBase的存储目录；hbase.cluster.distributed设置集群处于分布式模式；hbase.zookeeper.quorum设置Zookeeper节点的主机名，它的值个数必须是奇数；hbase.master.maxclockskew是用来防止HBase节点之间时间不一致造成regionserver启动失败，默认值是30000；hbase.zookeeper.property.dataDir设置Zookeeper的目录，默认为/tmp； 注：属性hbase.master的值可为IP地址形式，如hdfs://192.168.9.113:60000，但建议使用主机名。 3）regionservers123vi /opt/hbase-1.1.2/conf/regionserversycf-114ycf-115 注：跟/opt/hadoop2/hadoop-2.7.1/etc/hadoop/slaves保持一致。如果IP地址经常发生改变，建议用各主机主机名代替，slaves文件也一样。 4）backup-masters12vi /opt/hbase-1.1.2/conf/backup-mastersycf-115 注：此文件用于配置backup master，根据实际需要配置 3、替换HBase使用的Hadoop jar包1）查看12cd $&#123;HBASE_HOME&#125;/libfind ./ -name &apos;hadoop*jar&apos; 2）执行替换cd ${HBASE_HOME}/lib编辑脚本12345678vi f.shfind -name &quot;hadoop*jar&quot; | sed &apos;s/2.5.1/2.7.6/g&apos; | sed &apos;s/\\.\\///g&apos; &gt; f.logrm -rf ./hadoop*jarcat ./f.log | while read Linedofind $&#123;HADOOP_HOME&#125; -name &quot;$Line&quot; | xargs -i cp &#123;&#125; ./donerm -rf ./f.log 执行脚本sh f.shrm f.sh 4、复制HBase到其他从节点使用scp命令12scp -r /opt/hbase-1.1.2 hadoop@ ycf-114:/opt/scp -r /opt/hbase-1.1.2 hadoop@ ycf-115:/opt/ 5、启动与停止（主节点执行）/opt/hbase-1.1.2/bin启动：start-hbase.sh停止：stop-hbase.sh 6、查看进程1）主节点查看进程12345678[hadoop@ycf-113 logs]$ jps2624 JournalNode13944 YarnChild14121 Jps2519 DataNode6851 NodeManager2028 QuorumPeerMain7848 HMaster 2）从节点查看进程12345678[hadoop@ycf-114 bin]$ jps17617 Jps3628 NameNode4510 HRegionServer4160 DFSZKFailoverController3745 DataNode14013 ResourceManager3959 JournalNode 7、查看状态使用浏览器输入以下地址：Masterhttp://192.168.9.113:16010/ RegionServerhttp://192.168.9.114: 16030http://192.168.9.115: 16030 四、测试1、进入hbase的命令行管理界面123456[[hadoop@ycf-113 lib]$ hbase shellHBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.1.2, rcc2b70cf03e3378800661ec5cab11eb43fafe0fc, Wed Aug 26 20:11:27 PDT 2015hbase(main):001:0&gt; 2、新建数据表，并插入3条记录1234567891011121314hbase(main):001:0&gt;create &apos;test&apos;, &apos;cf&apos;0 row(s) in 55.7580 seconds=&gt; Hbase::Table - testhbase(main):002:0&gt; listTABLE test 1 row(s) in 4.8800 seconds=&gt; [&quot;test&quot;]hbase(main):003:0&gt; put &apos;test&apos;, &apos;row1&apos;, &apos;cf:a&apos;, &apos;value1&apos;0 row(s) in 6.7170 secondshbase(main):004:0&gt; put &apos;test&apos;, &apos;row2&apos;, &apos;cf:b&apos;, &apos;value2&apos;0 row(s) in 0.1410 secondshbase(main):005:0&gt; put &apos;test&apos;, &apos;row3&apos;, &apos;cf:c&apos;, &apos;value3&apos;0 row(s) in 0.5660 seconds 3、查看插入的数据123456hbase(main):006:0&gt; scan &apos;test&apos; ROW COLUMN+CELL row1 column=cf:a, timestamp=1421122693794, value=value1 row2 column=cf:b, timestamp=1421122702532, value=value2 row3 column=cf:c, timestamp=1421122711873, value=value3 3 row(s) in 2.7380 seconds 4、读取单条记录1234hbase(main):007:0&gt; get &apos;test&apos;, &apos;row1&apos; COLUMN CELL cf:a timestamp=1421122693794, value=value1 1 row(s) in 0.8210 seconds 5、停用并删除数据表1234hbase(main):008:0&gt; disable &apos;test&apos; 0 row(s) in 1.0930 seconds hbase(main):009:0&gt; drop &apos;test&apos; 0 row(s) in 0.0770 seconds 6、退出1hbase(main):010:0&gt; quit","tags":[{"name":"hbase","slug":"hbase","permalink":"www.hzzzy.top/tags/hbase/"}]},{"title":"logstash安装","date":"2018-05-08T03:42:31.000Z","path":"2018/05/08/logstash-install/","text":"一 简介 Logstash是一个接收、处理、转发日志的工具。支持系统日志、webserver日志、错误日志、应用日志等所有可以抛出来的日志类型。在一个典型的使用场景下(ELK)：用Elasticsearch作为后台数据的存储，kibana用来前端的报表展示。Logstash在其过程中担任搬运工的角色，它为数据存储，报表查询和日志解析创建了一个功能强大的管道链。Logstash提供了多种多样的input、filters、codecs和output组件，让使用者轻松实现强大的功能。Logstash不只是一个input|filter|output的数据流，而是一个input|decode|filter|encode|output的数据流。 二 安装Logstash2.1 下载12cd /optwget https://download.elastic.co/logstash/logstash/logstash-2.1.1.tar.gz 2.2 解压缩1tar xzvf logstash-2.1.1.tar.gz 2.3 测试12345678910111213141516[root@kafka-01 logstash-2.1.1]# bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos;helloworldSettings: Default filter workers: 4Logstash startup completed2015-12-09T02:35:27.392Z kafka-01 helloworld[root@kafka-01 logstash-2.1.1]# bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123; codec =&gt; rubydebug &#125; &#125;&apos;helloSettings: Default filter workers: 4Logstash startup completed&#123; &quot;message&quot; =&gt; &quot;hello&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; &quot;2015-12-09T02:37:41.405Z&quot;, &quot;host&quot; =&gt; &quot;kafka-01&quot;&#125; 2.4 其他安装方式（Redhat 平台）1234567891011rpm --import https://packages.elastic.co/GPG-KEY-elasticsearchcat &gt; /etc/yum.repos.d/logstash.repo &lt;&lt;EOF[logstash-2.1]name=Logstash repository for 2.1.x packagesbaseurl=http://packages.elastic.co/logstash/2.1/centosgpgcheck=1gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearchenabled=1EOFyum clean allyum install logstash 将会下载logstash-2.1.1-1.noarch.rpm并安装，但建议上官网自行下载RPM包，速度或许比较快，或者直接使用第一种方式安装。注意此安装方式会创建/etc/init.d/logstash文件及/etc/logstash/目录，前者设置开机启动，后者为默认logstash配置文件存放路径。安装完毕后，执行：service logstash start启动logstash服务（经测试，没有启动服务也能运行 bin/logstash -e ‘input { stdin { } } output { stdout {} }’）。 三 logstash配置语言3.1 区段(section)Logstash用{}来定义区域。区域内可以包括插件区域定义，你可以在一个区域内定义多个插件。插件区域内则可以定义键值对设置。示例如下：input { stdin {} syslog {}} 3.2 数据类型Logstash支持少量的数据值类型： booldebug =&gt; true stringhost =&gt; “hostname” numberport =&gt; 514 arraymatch =&gt; [“datetime”, “UNIX”, “ISO8601”] hashoptions =&gt; { key1 =&gt; “value1”, key2 =&gt; “value2”} 注意：如果你用的版本低于 1.2.0，哈希的语法跟数组是一样的，像下面这样写：match =&gt; [ “field1”, “pattern1”, “field2”, “pattern2” ] 3.3 字段引用(field reference)如果你想在Logstash配置中使用字段的值，只需要把字段的名字写在中括号[]里就行了，这就叫字段引用。 对于嵌套字段(也就是多维哈希表，或者叫哈希的哈希)，每层的字段名都写在[]里就可以了。比如，你可以从geoip里这样获取longitude值(是的，这是个笨办法，实际上有单独的字段专门存这个数据的)：[geoip][location][0] Logstash还支持变量内插，在字符串里使用字段引用的方法是这样：“the longitude is %{[geoip][location][0]}” 3.4 条件判断(condition)Logstash从 1.3.0 版开始支持条件判断和表达式。 表达式支持下面这些操作符： equality, etc: ==, !=, &lt;, &gt;, &lt;=, &gt;=regexp: =~, !~inclusion: in, not inboolean: and, or, nand, xorunary: !()通常来说，你都会在表达式里用到字段引用。比如： if “_grokparsefailure” not in [tags] {} else if [status] !~ /^2\\d\\d/ and [url] == “/noc.gif” {} else {} 3.5 命令行参数-e ：命令行执行–config或-f ：执行时指定配置文件–configtest或-t ：测试Logstash读取到的配置文件语法是否能正常解析–log或-l ：指定日志存储–filterworkers或-w ：指定工作线程–pluginpath或-P ：加载自定义插件–verbose ：输出一定的调试日志–debug ：输出更多的调试日志 四 配置文件4.1 组件介绍1） InputInput是指日志数据传输到Logstash中，常见的input如下：stdin：标准输入(常见的用途就是调试)file：从文件系统中读取一个文件redis：从redis服务器读取数据，支持channel(发布订阅)、pattern_channel(并行发布订阅)和list模式。 2） FilterFilter在Logstash处理链中担任中间处理组件。他们经常被组合起来实现一些特定的行为来，处理匹配特定规则的事件流。常见的filter如下：grok：解析无规则的文字并转化为有结构的格式。Grok 是目前最好的方式来将无结构的数据转换为有结构可查询的数据。有120多种匹配规则，会有一种满足你的需要。date：转换日志记录中的时间字符串，变成LogStash::Timestamp对象，然后转存到@timestamp字段里。mutate：允许改变输入的文档，包括类型转换，字符串处理和字段处理等。。geoip：是最常见的免费IP地址归类查询库，同时也有收费版可以采购。GeoIP库可以根据IP地址提供对应的地域信息，包括国别，省市，经纬度等。split：把一行数据，拆分成多个事件。drop：丢弃一部分events不进行处理，例如：debug events。clone：拷贝event，这个过程中也可以添加或移除字段。 3） Outputoutput是logstash处理管道的最末端组件。一个event可以在处理过程中经过多重输出，但是一旦所有的output都执行结束，这个event也就完成生命周期。常用的output如下：stdout：标准输出(常见的用途就是调试)file：将event数据保存到文件中。elasticsearch：ESgraphite：将event数据发送到图形化组件中，一个很流行的开源存储图形化展示的组件。statsd：statsd是一个统计服务，比如技术和时间统计，通过udp通讯，聚合一个或者多个后台服务 4） Codecodec是基于数据流的过滤器，它可以作为input，output的一部分配置。Codec可以帮助你轻松的分割发送过来已经被序列化的数据。流行的codec包括json：使用json格式对数据进行编码/解码multiline：将汇多个事件中数据汇总为一个单一的行plain(text)： 4.2 使用配置文件vi logstash-simple.conf添加以下内容：input { stdin { } }output { elasticsearch { host =&gt; localhost } stdout { codec =&gt; rubydebug }} 4.3 测试bin/logstash -f logstash-simple.conf -t 4.4 执行命令bin/logstash -f logstash-simple.conf 后台运行nohup bin/logstash -f logstash-simple.conf &amp; 五 高级配置详情请查阅以下链接内容：https://www.elastic.co/guide/en/logstash/current/index.html","tags":[{"name":"logstash","slug":"logstash","permalink":"www.hzzzy.top/tags/logstash/"}]},{"title":"sqoop安装","date":"2018-04-10T03:28:13.000Z","path":"2018/04/10/sqoop-install/","text":"Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。对于某些NoSQL数据库它也提供了连接器。Sqoop，类似于其他ETL工具，使用元数据模型来判断数据类型并在数据从数据源转移到Hadoop时确保类型安全的数据处理。Sqoop专为大数据批量传输设计，能够分割数据集并创建Hadoop任务来处理每个区块。 1.安装我们使用的版本是sqoop-1.4.5.bin__hadoop-2.0.4-alpha.tar.gz，打算安装在/home/hadoop目录下。首先就是解压缩，重命名为sqoop（为保留版本号，本次安装未重命名），然后在文件/etc/profile中设置环境变量SQOOP_HOME和PATH。把mysql的jdbc驱动mysql-connector-java-5.1.26-bin.jar复制到sqoop项目的lib目录下。 2.重命名配置文件在${SQOOP_HOME}/conf中执行命令1cp sqoop-env-template.sh sqoop-env.sh 在conf目录下，有两个文件sqoop-site.xml和sqoop-site-template.xml内容是完全一样的，不必在意，我们只关心sqoop-site.xml即可。 3.修改配置文件sqoop-env.sh内容如下123456789101112131415#Set path to where bin/hadoop is availableexport HADOOP_COMMON_HOME=/home/hadoop/hadoop-2.4.0#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/home/hadoop/hadoop-2.4.0#set the path to where bin/hbase is availableexport HBASE_HOME=/home/hadoop/hbase-0.98.9-hadoop2#Set the path to where bin/hive is availableexport HIVE_HOME=/home/hadoop/apache-hive-0.14.0-bin#Set the path for where zookeper config dir is#export ZOOCFGDIR=#由于主节点没有安装zookeeper，暂不配置ZOOCFGDIR 好了，搞定了，下面就可以运行了。 4.sqoop 使用1234567891011121314151617181920[hadoop@master bin]$ sqoop help15/01/16 15:05:11 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5usage: sqoop COMMAND [ARGS]Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version informationSee &apos;sqoop help COMMAND&apos; for information on a specific command. 1)列出mysql数据库中的所有数据库命令sqoop list-databases –connect jdbc:mysql://master:3306/ –username root –password root 2)连接mysql并列出数据库中的表命令sqoop list-tables –connect jdbc:mysql://master:3306/test –username root –password root命令中的test为mysql数据库中的test数据库名称 username password分别为mysql数据库的用户密码 3)将关系型数据的表结构复制到hive中sqoop create-hive-table –connect jdbc:mysql://master:3306/test –table mytable –username root –password root –hive-table test其中 –table username为mysql中的数据库test中的表 –hive-table test 为hive中新建的表名称 4)从关系数据库导入文件到hive中（可以是步骤3中已复制过去的表结构，也可以是未复制的，这样就会新建一个表）sqoop import –connect jdbc:mysql://master:3306/test –username root –password root –table mytable –hive-table test –hive-import 注意：如果jdbc:mysql://master:3306这里使用localhost而不是主机名或者IP地址，会报Communications link failure错误。详细解答见：http://f.dataguru.cn/thread-127146-1-2.html","tags":[{"name":"sqoop","slug":"sqoop","permalink":"www.hzzzy.top/tags/sqoop/"}]},{"title":"JavaWeb 过滤器跟拦截器的区别和使用","date":"2018-04-08T01:48:58.000Z","path":"2018/04/08/filterandlistener/","text":"一、过滤器1.什么是过滤器？ 过滤器是一个程序，它先于与之相关的servlet或JSP页面运行在服务器上。过滤器可附加到一个或多个servlet或JSP页面上，并且可以检查进入这些资源的请求信息。在这之后，过滤器可以作如下的选择：①以常规的方式调用资源（即，调用servlet或JSP页面）。②利用修改过的请求信息调用资源。③调用资源，但在发送响应到客户机前对其进行修改。④阻止该资源调用，代之以转到其他的资源，返回一个特定的状态代码或生成替换输出。 2.Servlet过滤器的基本原理： 在Servlet作为过滤器使用时，它可以对客户的请求进行处理。处理完成后，它会交给下一个过滤器处理，这样，客户的请求在过滤链里逐个处理，直到请求发送到目标为止。例如，某网站里有提交“修改的注册信息”的网页，当用户填写完修改信息并提交后，服务器在进行处理时需要做两项工作：判断客户端的会话是否有效；对提交的数据进行统一编码。这两项工作可以在由两个过滤器组成的过滤链里进行处理。当过滤器处理成功后，把提交的数据发送到最终目标；如果过滤器处理不成功，将把视图派发到指定的错误页面。 3.示例代码123456789在web.xml里面配置自定义的过滤器 &lt;filter&gt; &lt;filter-name&gt;Redirect Filter&lt;/filter-name&gt; &lt;filter-class&gt;com.xx.filter.RedirectFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;Redirect Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/xx/xx/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 12345678910111213141516171819202122232425如何编写自定义的过滤器 public class RedirectFilter implements Filter &#123; public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws IOException, ServletException &#123; // 获取URL Long startTime = null; if (log.isDebugEnabled()) &#123; startTime = System.currentTimeMillis(); &#125; HttpServletRequest httpRequest = (HttpServletRequest) request; String url = httpRequest.getRequestURL().toString(); if (url == null || url.trim().length() == 0) &#123; return; &#125; if (url.indexOf(luceneCreateMapping) != -1 || url.indexOf(luceneSearchMapping) != -1) &#123; doFilterForxxx(request, response, url); &#125; else &#123; doxxxx(request, response, url); &#125; if (log.isDebugEnabled()) &#123; long endTime = System.currentTimeMillis(); Thread currentThread = Thread.currentThread(); String threadName = currentThread.getName(); log.debug(&quot;[&quot; + threadName + &quot;]&quot; + &quot;&lt; &quot; + this.getClass().getName() + &quot; &quot; + url + &quot; &quot; + (endTime - startTime) + &quot; ms&quot;); &#125; // 激活下一个Filter filterChain.doFilter(request, response); &#125; &#125; 123456789示例2 // 填充100个带有随机字母标签的球 List&lt;String&gt; array = new ArrayList&lt;&gt;(); Random r = new Random(); String[] balls = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;; for (int i = 0; i &lt; 100; i++) array.add(balls[r.nextInt(3)]); // 只拿出B的来。不明白的自行学习Java 8 array = array.stream().filter(ball -&gt; ball.equals(&quot;B&quot;)).collect(Collectors.toList()); 二、拦截器1.什么是拦截器？ 拦截器，在AOP（Aspect-Oriented Programming）中用于在某个方法或字段被访问之前，进行拦截然后在之前或之后加入某些操作。拦截是AOP的一种实现策略。在Webwork的中文文档的解释为——拦截器是动态拦截Action调用的对象。它提供了一种机制可以使开发者可以定义在一个action执行的前后执行的代码，也可以在一个action执行前阻止其执行。同时也是提供了一种可以提取action中可重用的部分的方式。谈到拦截器，还有一个词大家应该知道——拦截器链（Interceptor Chain，在Struts 2中称为拦截器栈 Interceptor Stack）。拦截器链就是将拦截器按一定的顺序联结成一条链。在访问被拦截的方法或字段时，拦截器链中的拦截器就会按其之前定义的顺序被调用。 2.拦截器的实现原理： 大部分时候，拦截器方法都是通过代理的方式来调用的。Struts 2的拦截器实现相对简单。当请求到达Struts 2的ServletDispatcher时，Struts 2会查找配置文件，并根据其配置实例化相对的拦截器对象，然后串成一个列表（list），最后一个一个地调用列表中的拦截器。 3.示例代码12345678在xml文件中如何定义拦截器 &lt;interceptors&gt; &lt;interceptor name=&quot;filterIPInterceptor&quot; class=&quot;com.xxxx.web.FilterIPActionInterceptor&quot; /&gt; &lt;interceptor-stack name=&quot;filterIPStack&quot;&gt; &lt;interceptor-ref name=&quot;defaultStack&quot; /&gt; &lt;interceptor-ref name=&quot;filterIPInterceptor&quot; /&gt; &lt;/interceptor-stack&gt; &lt;/interceptors&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041怎么编写自定义拦截器 public class FilterIPActionInterceptor extends AbstractInterceptor&#123; /** 日志控制. */ private final Log log = LogFactory.getLog(getClass()); /** * @see com.opensymphony.xwork2.interceptor.AbstractInterceptor#intercept(com.opensymphony.xwork2.ActionInvocation) */ @Override @SuppressWarnings(&quot;unchecked&quot;) public String intercept(ActionInvocation invocation) throws Exception &#123; String result = null; // 获得当前方法名. String methodName = invocation.getInvocationContext().getName(); String currIp = null; try &#123; if (invocation.getAction() instanceof PortletAction) &#123; PortletAction action = (PortletAction) invocation.getAction(); currIp = action.getRequest().getRemoteAddr(); &#125; String ip = ApplicationResource.getHotValue(&quot;ALLOW_CACHE_IP&quot;); if (StringUtils.isBlank(ip) || StringUtils.isBlank(currIp)) &#123; log.error(&quot;允许刷新的IP不存在或当前请求的IP非法.&quot;); throw new NoAllowIPException(); &#125; else &#123; String[] ips = ip.split(&quot;,&quot;); boolean errorIp = true; for (String s : ips) &#123; if (s.equals(currIp)) errorIp = false; &#125; // 判断IP if (errorIp) throw new NoAllowIPException(); &#125; result = invocation.invoke();//调用被拦截的方法 &#125; catch (Exception e) &#123; log.error(&quot;异常类名:&quot; + invocation.getAction().getClass()); log.error(&quot;异常方法:&quot; + methodName, e); throw e; &#125; return result; &#125; &#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455示例2class River &#123; // 流量 int volume; // 总鱼数 int numFish; &#125; class PowerGenerator &#123; double generate (int volume) &#123; // 假设每一百立方米水发一度电 return volume / 100; &#125; &#125; interface Interceptor &#123; void intercept (River river); &#125; class SomeInterceptor implements Interceptor &#123; PowerGenerator generator = new PowerGenerator(); @Override public void intercept (River river) &#123; // 消耗了1000立方米水来发电 int waterUsed = 1000; // 水量减少了1000。请不要跟我讨论水电站原理的问题， // 我就那么一比方 river.volume -= waterUsed; // 发电 generator.generate (waterUsed); // 拦截所有的鱼 river.numFish = 0; &#125; &#125; class RiverController &#123; Interceptor interceptor; void flow(River river) &#123; // 源头积累下来的水量和鱼 river.volume += 100000; river.numFish += 1000 ; // 经过了大坝 interceptor.intercept(river); // 下了点雨 river.volume += 1000 &#125; void setInterceptor (Interceptor interceptor) &#123; this.interceptor = interceptor &#125; &#125; class Main &#123; public static void main (String args[]) &#123; RiverController rc = new RiverController; Interceptor inter = new SomeInterceptor(); // 这一步通常叫做控制反转或依赖注入，其实也没啥子 rc.setInterceptor(inter); rc.flow(new River()); &#125; &#125; 拦截器与过滤器的区别 ： 拦截器是基于java的反射机制的，而过滤器是基于函数回调。 拦截器不依赖与servlet容器，过滤器依赖与servlet容器。 拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。 拦截器可以访问action上下文、值栈里的对象，而过滤器不能访问。 在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次拦截器的代 码实现。 Filter基于回调函数，我们需要实现的filter接口中doFilter方法就是回调函数，而interceptor则基于 java本身的反射机制,这是两者最本质的区别。 Filter是依赖于servlet容器的，即只能在servlet容器中执行，很显然没有servlet容器就无法来回调 doFilter方法。而interceptor与servlet容器无关。 Filter的过滤范围比Interceptor大,Filter除了过滤请求外通过通配符可以保护页面，图片，文件等等， 而Interceptor只能过滤请求。 Filter的过滤例外一般是在加载的时候在init方法声明,而Interceptor可以通过在xml声明是guest请求还 是user请求来辨别是否过滤 三、监听器1.监听器（Listener）：当一个事件发生的时候，你希望获得这个事件发生的详细信息，而并不想干预这个事件本身的进程，这就要用到监听器。2.示例代码1234567891011121314151617181920212223242526272829// 监听器 interface BedListener &#123; // 监听器在参数中收到了某个事件，而这个事件往往是只读的 // 监听器的方法名通常以&quot;on&quot;开头 void onBedSound (String sound); &#125; class Neighbor &#123; BedListener listener; // 依然是所谓控制反转 setListener (BedListener listener) &#123; this.listener = listener; &#125; void doInterestingStuff () &#123; // 根据当地法律法规，部分内容无法显示 // 将事件发送给监听器 listener.onBedSound(&quot;嘿咻&quot;); listener.onBedSound(&quot;oyeah&quot;); &#125; &#125; class Main &#123; public static void main (String args[]) &#123; Neighbor n = new Neighbor(); n.setListener (sound -&gt; generatePower()); n.doInterestingStuff(); &#125; private static void generatePower() &#123; // 根据当地法律法规，部分内容无法显示 &#125; &#125;","tags":[{"name":"Filter","slug":"Filter","permalink":"www.hzzzy.top/tags/Filter/"}]},{"title":"Spark基本架构及原理","date":"2018-01-28T03:19:42.000Z","path":"2018/01/28/spark/","text":"Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架，与Hadoop和Storm等其它大数据技术相比，Spark有如下优势： 1、Spark提供了一个全面、统一的框架用于管理各种有着不同性质（文本数据、图表数据等）的数据集和批量数据/实时的流数据的大数据处理需求； 2、Spark是基于内存计算框架，运行速度提升100倍，甚至在磁盘上的运行速度提升10倍。 一、基本架构 Spark Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的。Spark SQL：提供通过Apache Hive的HQL查询语言与Spark进行交互的API；每个数据库表被当做一个RDD，Spark SQL查询被转换为Spark操作。Spark Streaming：对实时数据进行处理和控制；基于RDD操作。MLlib：一个常用机器学习算法库，算法被实现为对RDD的Spark操作。GraphX：控制图、并行图操作和计算的一组算法和工具的集合。 ClusterManager：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器（ResourceManager）。Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。Driver：运行Application的main()函数。Executor：执行器，是为某个Application运行在Worker Node上的一个进程。 二、Spark运行流程 1、构建Spark Application的运行环境，启动SparkContext；2、SparkContext向资源管理器（可以是Standalone、Mesos、YARN）申请运行Executor资源，并启动StandaloneExecutorBackend；3、Executor向SparkContext申请Task；4、SparkContext将应用程序分发给Executor；5、SparkContext构建DAG图，将DAG图分解成Stage，将Taskset发送给Task Scheduler，最后由Task Scheduler将Task发送给Executor运行；6、Task在Executor运行，运行完释放所有资源。 Spark运行特点： 1、每个Application获取专属的Executor进程，该进程在Application期间一直驻留，并以多线程的方式运行Task。这意味着Spark Application不能跨应用程序共享数据，除非将数据写入外部存储系统；2、Spark与资源管理器无关，只要能够获取Executor进程并能保持互相通信即可；3、提交SparkContext的Client应该靠近Worker节点（运行Executor的节点），最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；4、Task采用数据本地性和推测执行的执行机制。 三、常用术语Application：Application都是指用户编写的Spark应用程序，其中包括一个Driver功能代码和分布在集群中多个节点上运行的Executor代码；Driver：运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中由SparkContext负责与Cluster Manager通信，进行资源申请、任务的分配和监控等，当Executor运行完毕后，Driver同时负责SparkContext关闭，通常用SparkContext代表Driver；Executor：某个Application运行在worker节点上的一个进程，该进程负责运行某些Task，并且负责将数据存储到内存或磁盘上，每个Application都有各自独立的一批Executor，在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutorBackend。一个CoarseGrainedExecutorBackend有且仅有一个Executor对象，负责将Task包装成TaskRunner，并从线程池中抽取一个空闲线程运行Task，每个CoarseGrainedExecutorBackend能并行运行Task数量取决于分配给它的CPU个数。ClusterManager：指的是集群上获取资源的外部服务。目前有三种类型： 1、Standalone：spark原生的资源管理，有master负责资源的分配； 2、Apache Mesos：与Hadoop MR兼容性良好的一种资源调度框架； 3、Hadoop Yarn：Yarn的ResourceManager；Worker：集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark Yarn模式下就是NodeManager节点。Task：被送到某个Executor的工作单元，和Hadoop MR中的MapTask和ReduceTask 概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责。Job：包含多个Task组成的并行计算，往往由Spark Action触发生成，一个Application中往往会产生多个Job。Stage：每个Job会被拆分成多组Task作为一个Taskset，其名称为Stage，Stage的划分和调度是由DAGScheduler来负责，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方。DAGScheduler：根据Job构建基于Stage的DAG（有向无环图），并提交Stage给TaskScheduler。其划分Stage的依据是RDD之间的依赖关系找出开销最小的调度方法。TaskScheduler：将Taskset提交给Worker运行，每个Executor运行那些Task由它分配，TaskScheduler维护所有Taskset，当Executor向Driver发生心跳时，TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task的运行标签，重试失败的Task。在不同运行模式中任务调度器具体为： 1、Spark on Standalone模式为TaskScheduler 2、Yarn-Client模式为YarnClientClusterScheduler 3、Yarn-Cluster模式为YarnClusterScheduler Job=多个Stage，Stage=多个同种Task，Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency。 四、运行模式Spark的运行模式多种多样，灵活可变，部署在单机上时，既可以用本地模式运行，也可以用伪分布式模式运行，而当以分布式集群方式部署时，有众多的运行模式可供选择。底层的资源调度可依赖外部资源调度框架，也可以使用Spark内建的Standalone模式，对于外部资源调度框架，目前相对稳定的有Mesos、Hadoop YARN两种。本地模式：常用于本地开发测试，分为local和local cluster。Standalone：独立集群运行模式 1、Spark自带的资源调度框架； 2、采用Master/Slaves的典型结构，选用ZooKeeper来实现Master的HA； 3、架构图如下：该模式主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Spark的Job或者在IDEA开发工具上使用“ new SparkConf.setManager(“spark://master:7077”) ”方式运行Spark任务时，Driver是运行在本地Client端上。其运行过程如下图： 1、SparkContext连接到Master，向Master注册并申请资源（CPU Core和Memory）；2、Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；3、StandaloneExecutorBackend向SparkContext注册；4、SparkContext将Application代码发送给StandaloneExecutorBackend，并且SparkContext解释Application代码，构建DAG图，并提交给DAGScheduler分解成Stage（当碰到Action操作时，就会催生Job，每个Job中包含1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），然后以Stage（或者称为Taskset）提交给TaskScheduler，TaskScheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；5、StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成；6、所有Task完成后，SparkContext向Master注销，释放资源。Yarn：资源调度管理器（详情请参考《YARN框架原理及运行机制》）Spark on Yarn模式根据Driver在集群中的位置分为两种模式：Yarn-Client和Yarn-Cluster（或者称为Yarn-Standalone）。Yarn-Client模式中，Driver是运行在本地客户端上，可以通过WebUI访问Driver的状态，默认是http://hostname:4040访问。Yarn-Client模式： 1、Spark Yarn Client向Yarn的ResourceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler等，由于选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和CoarseGrainedExecutorBackend；2、ResourceManager收到请求后，在集群中选择一个NodeManager为该程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与Yarn-Cluster区别的是该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派；3、Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）。4、一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task；5、Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而可以在任务失败时重启任务；6、应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。Yarn-Cluster模式：在Yarn-Cluster模式中，当用户向Yarn提交一个应用程序后，Yarn将分两个阶段运行该程序：1、第一阶段把Spark的Driver作为一个ApplicationMaster在Yarn集群中先启动；2、第二阶段由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程。 1、Spark Yarn Client向Yarn提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor运行的程序等；2、ResourceManager收到请求后，在集群中选择一个NodeManager为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化；3、ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控他们的运行状态直到运行结束；4、一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task；5、ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而在任务失败时重启任务；6、应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。","tags":[{"name":"spark","slug":"spark","permalink":"www.hzzzy.top/tags/spark/"}]},{"title":"ubuntu下安装微信小程序开发工具","date":"2018-01-24T01:18:57.000Z","path":"2018/01/24/wxcode/","text":"微信小程序官网提供的下载程序版本只有windows和mac的,并没有linux系统的安装包由于本人电脑装的是ubuntu系统,要开发小程序,得安装相应的工具. 步骤:1.安装winesudo apt-get install wine 2.安装nwjs-sdk2.1 下载linux版nwjs-sdkwget https://dl.nwjs.io/v0.25.4/nwjs-sdk-v0.25.4-linux-x64.tar.gz2.2 解压nwjs-sdktar xvf nwjs-sdk-v0.25.4-linux-x64.tar.gz2.3 切换到nwjs-sdk对应的目录cd nwjs-sdk-v0.25.4-linux-x642.4 启动nwjs-sdk./nw (测试是否能正常运行,测试正常就关掉) 3 安装微信开发工具包(基于nwjs-sdk)3.1 获取微信开发工具包git clone https://github.com/cytle/wechat_web_devtools.git3.2. 切换到wechat_web_devtools目录cd wechat_web_devtools3.3 复制微信开发工具包（package.nw在wechat_web_devtools目录下）到nwjs-sdk目录下cp package.nw ~/development/nwjs-sdk-v0.25.4-linux-x64/ -rf3.4 启动./nw 大功告成! `","tags":[{"name":"工具","slug":"工具","permalink":"www.hzzzy.top/tags/工具/"}]},{"title":"ubuntu下创建快捷方式(启动器)","date":"2018-01-05T01:53:33.000Z","path":"2018/01/05/ubuntu-desktop/","text":"前言使用tar解压安装的软件,例如eclipse,SQL Devoloper,pycharm等等这些软件,安装后并没有快捷方式,启动器快捷中也不会有.为了方便启动软件,不需要进到安装目录使用***.sh命令启用,创建软件快捷方式,能让你快速启动软件.下面介绍两种创建方式: 方法一:1、搜索进入“启动应用程序” 2、在启动用程序首选项中选择“添加” 3、设置完名称和命令（路径）后，点击“添加”，启动器就被添加到列表里了4、用鼠标选中刚添加的启动器，将其拖到桌面，这时就会发现产生了一个同名的.desktop文件 5、右击新建的起动器，选择属性，在权限选项卡中勾选——执行：允许以程序执行文件 6、在基本选项卡中点击图标，可以设置启动器的桌面图标。(这里感觉有点坑，Navicat里没找到icon，自己下了一个) 7、想放到启动栏的话直接拖过去就好了，不要打开之后点”锁定到启动器” 方法二:每个图标对应/usr/share/applications当中的一个配置文件（文件后缀为.desktop)。所以要在dash home中 添加一个自定义程序启动器，需要在该文件目录下创建对应的配置文件。配置文件的语法，参考该目录下的配置文件即可明白： 1.打开终端，切换用root账户下，接着在/usr/share/applications目录下创建相应的启动配置文件（注意：一定要和启动项同名） 2.desktop的格式如下：123456789[Desktop Entry]//文件头Encoding=UTF-8 //编码方式Name=XXX//应用程序名称Comment=comment//提示Exec= //菜单执行的命令或程序路径Icon=//显示在菜单项中的图标，可以为空Terminal=false //是否使用终端Type=Application //分类Categories= //菜单所属类别，可以确定该菜单的位置 下面以eclipse为例:12345678910[Desktop Entry]Encoding=UTF-8Name=Eclipse PlatfromComment=Eclipse IDE //应用程序名称Exec=/opt/eclipse/eclipse //我的eclipse安装目录下的启动命令Icon=/opt/eclipse/icon.xpm //我的eclipse安装目录下的图标Terminal=falseStartupNotify=trueType=ApplicationCategories=Application;Development;","tags":[{"name":"问题","slug":"问题","permalink":"www.hzzzy.top/tags/问题/"}]},{"title":"oracle重复数据去重、删除重复数据、去重统计","date":"2017-12-08T07:10:47.000Z","path":"2017/12/08/oracle-distinct/","text":"我们通常使用distinct关键字来去除重复记录，其实，除了distinct，利用rowid，group by也可以实现去重。 1、DISTINCT去重1select distinct f1,f2 from table; 2、group by去重1select f1,f2 from table group by(f1,f2); 3、查找表中多余的重复记录，重复记录是根据单个字段（Id）来判断1select * from table where Id in (select Id from table group by Id having count(Id) &gt; 1); 4、查找表中多余的重复记录（多个字段）1select * from table a where (a.Id,a.seq) in (select Id,seq from table group by Id,seq having count(*) &gt; 1); 5、删除表中多余的重复记录，重复记录是根据单个字段（Id）来判断，只留有rowid最小的记录1delete from table where (id) in ( select id from table group by id having count(id) &gt; 1) and rowid not in (select min(rowid) from table group by id having count(*) &gt; 1); 6、删除表中多余的重复记录（多个字段），只留有rowid最小的记录1delete from table a where (a.Id,a.seq) in (select Id,seq from table group by Id,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from tableroup by Id,seq having count(*)&gt;1); 7、查找表中多余的重复记录（多个字段），不包含rowid最小的记录1select * from table a where (a.Id,a.seq) in (select Id,seq from table group by Id,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from table group by Id,seq having count(*)&gt;1); 8、过滤重复数据统计数目1select count(distinct f1) from table;","tags":[{"name":"oracle","slug":"oracle","permalink":"www.hzzzy.top/tags/oracle/"}]},{"title":"springboot-schedule使用","date":"2017-12-02T06:54:07.000Z","path":"2017/12/02/springboot-schedule/","text":"本文介绍schedule和quartz在springboot中的使用 schedule的使用在application.properties中配置定时跑的时间12#每10秒跑一次job.second.cron=0/10 * * * * * 编写定时器：123456789101112@Component@EnableSchedulingpublic class TestTask &#123; public static int data = 0; @Scheduled(cron = &quot;$&#123;job.second.cron&#125;&quot;) public void run() &#123; System.out.println(&quot;--data=&quot; + data); data += 1; &#125;&#125; 启动程序，结果如下：123--data=0--data=1--data=2 2. quartz 使用工程中引进quartz：12345&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 然后编写的配置类:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Configurationpublic class QuartzConfigration &#123; @Bean(name = &quot;jobDetail&quot;) public MethodInvokingJobDetailFactoryBean detailFactoryBean(TestTask task) &#123; // ScheduleTask为需要执行的任务 MethodInvokingJobDetailFactoryBean jobDetail = new MethodInvokingJobDetailFactoryBean(); /* * 是否并发执行 例如每5s执行一次任务，但是当前任务还没有执行完，就已经过了5s了， * 如果此处为true，则下一个任务会bing执行，如果此处为false，则下一个任务会等待上一个任务执行完后，再开始执行 */ jobDetail.setConcurrent(true); jobDetail.setName(&quot;scheduler&quot;);// 设置任务的名字 jobDetail.setGroup(&quot;scheduler_group&quot;);// 设置任务的分组，这些属性都可以存储在数据库中，在多任务的时候使用 /* * 这两行代码表示执行task对象中的scheduleTest方法。定时执行的逻辑都在scheduleTest。 */ jobDetail.setTargetObject(task); jobDetail.setTargetMethod(&quot;exe&quot;);//定时器运行的方法 return jobDetail; &#125; @Bean(name = &quot;jobTrigger&quot;) public CronTriggerFactoryBean cronJobTrigger(MethodInvokingJobDetailFactoryBean jobDetail) &#123; CronTriggerFactoryBean tigger = new CronTriggerFactoryBean(); tigger.setJobDetail(jobDetail.getObject()); tigger.setCronExpression(&quot;0 20 15 * * ?&quot;); //每天15:20启动程序 // tigger.set tigger.setName(&quot;RT-Tigger&quot;);// trigger的name return tigger; &#125; @Bean(name = &quot;scheduler&quot;) public SchedulerFactoryBean schedulerFactory(Trigger cronJobTrigger) &#123; SchedulerFactoryBean bean = new SchedulerFactoryBean(); // 设置是否任意一个已定义的Job会覆盖现在的Job。默认为false，即已定义的Job不会覆盖现有的Job。 bean.setOverwriteExistingJobs(true); // 延时启动，应用启动5秒后 ，定时器才开始启动 bean.setStartupDelay(5); // 注册定时触发器 bean.setTriggers(cronJobTrigger); return bean; &#125; // 多任务时的Scheduler，动态设置Trigger。一个SchedulerFactoryBean可能会有多个Trigger @Bean(name = &quot;multitaskScheduler&quot;) public SchedulerFactoryBean schedulerFactoryBean() &#123; SchedulerFactoryBean schedulerFactoryBean = new SchedulerFactoryBean(); return schedulerFactoryBean; &#125;&#125; 有了定时器配置了，TestTask也明确了，运行的是TestTask的exe方法：1234567891011@Component@EnableSchedulingpublic class TestTask &#123; public static int data = 0; public void exe() &#123; System.out.println(&quot;--data=&quot; + data); data += 1; &#125;&#125; 结果如下:1--data=0 设置好了的定时器，如何动态改变定时时间呢？其实就是取定时器配置，然后修改就可以了：12345678910111213141516171819202122232425262728293031323334@Componentpublic class ChangeTime &#123; @Resource(name = &quot;jobDetail&quot;) private JobDetail jobDetail; @Resource(name = &quot;jobTrigger&quot;) private CronTrigger cronTrigger; @Resource(name = &quot;scheduler&quot;) private Scheduler scheduler; public void changeTime() &#123; //todo //更改的时间可以从应用配置取，也可从数据库取 String time = &quot;&quot;; if (!StringUtils.isBlank(time)) &#123; try &#123; CronTrigger trigger = (CronTrigger) scheduler.getTrigger(cronTrigger.getKey()); // String currentCron = trigger.getCronExpression(); // 当前Trigger使用的 CronScheduleBuilder scheduleBuilder = CronScheduleBuilder.cronSchedule(time); // 按新的cronExpression表达式重新构建trigger trigger = (CronTrigger) scheduler.getTrigger(cronTrigger.getKey()); trigger = trigger.getTriggerBuilder().withIdentity(cronTrigger.getKey()).withSchedule(scheduleBuilder) .build(); // 按新的trigger重新设置job执行 scheduler.rescheduleJob(cronTrigger.getKey(), trigger); &#125; catch (SchedulerException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;","tags":[{"name":"schedule","slug":"schedule","permalink":"www.hzzzy.top/tags/schedule/"}]},{"title":"一个实用的任务调度工具类-QuartzUtil","date":"2017-12-01T09:26:14.000Z","path":"2017/12/01/QuartzUtil/","text":"有时候我们需要定时启动一些模块，比如5秒一次启动，或一天启动一次下面的任务调度工具类满足你的需求。 基础知识 SchedulerFactory – 调度程序工厂 StdSchedulerFactory – Quartz默认的SchedulerFactory DirectSchedulerFactory – DirectSchedulerFactory是对SchedulerFactory的直接实现,通过它可以直接构建Scheduler、threadpool 等 ThreadExecutor / DefaultThreadExecutor – 内部线程操作对象 Scheduler – 调度器 StdScheduler – Quartz默认的Scheduler RemoteScheduler – 带有RMI功能的Scheduler JOB –任务对象 JobDetail – 他是实现轮询的一个的回调类，可将参数封装成JobDataMap对象,Quartz将任务的作业状态保存在JobDetail中. JobDataMap – JobDataMap用来报错由JobDetail传递过来的任务实例对象 Trigger –触发器 SimpleTrigger &lt;普通的Trigger&gt; – SimpleScheduleBuilder CronTrigger &lt;带Cron Like 表达式的Trigger&gt; – CronScheduleBuilder CalendarIntervalTrigger &lt;带日期触发的Trigger&gt; – CalendarIntervalScheduleBuilder DailyTimeIntervalTrigger &lt;按天触发的Trigger&gt; – DailyTimeIntervalScheduleBuilder maven 引用12345&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 工具类QuartzUtil类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178package com.hzzzy.task;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;import org.quartz.DateBuilder;import org.quartz.Job;import org.quartz.JobBuilder;import org.quartz.JobDetail;import org.quartz.JobKey;import org.quartz.SchedulerException;import org.quartz.SchedulerFactory;import org.quartz.SimpleScheduleBuilder;import org.quartz.Trigger;import org.quartz.TriggerBuilder;import org.quartz.TriggerKey;import org.quartz.impl.StdSchedulerFactory;/** * 任务调度公共类 * */public class QuartzUtil &#123;/** * 任务组 */private final static String JOB_GROUP_NAME = &quot;QUARTZ_JOBGROUP_LOACAL&quot;;/** * 触发器组 */private final static String TRIGGER_GROUP_NAME = &quot;QUARTZ_TRIGGERGROUP_LOACAL&quot;;public static SchedulerFactory sf;public static SchedulerFactory getSchedulerFactory() &#123; if (sf == null) &#123; sf = new StdSchedulerFactory(); &#125; return sf;&#125;/** * * @param jobName * 任务名 * @param triggerName * 触发器名 * @param jobClass * 执行任务的类 * @param seconds * 间隔时间 单位秒 * @param secondsLater * 启动后多少秒开始执行 单位秒 * @throws SchedulerException */public static void addJob(String jobName, String triggerName, Class&lt;? extends Job&gt; jobClass, int seconds,int secondsLater) throws SchedulerException &#123; // 创建一个SchedulerFactory工厂实例 // 通过SchedulerFactory构建Scheduler对象 getSchedulerFactory(); // Scheduler sche = sf.getScheduler(); // 用于描叙Job实现类及其他的一些静态信息，构建一个作业实例 JobDetail jobDetail = JobBuilder.newJob(jobClass).withIdentity(jobName, JOB_GROUP_NAME).build(); // 构建一个触发器，规定触发的规则 Trigger trigger = TriggerBuilder.newTrigger() // 创建一个新的TriggerBuilder来规范一个触发器 .withIdentity(triggerName, TRIGGER_GROUP_NAME) // 给触发器起一个名字和组名 .startAt(DateBuilder.futureDate(secondsLater, DateBuilder.IntervalUnit.SECOND)) .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(seconds).repeatForever()// 一直执行 ).build();// 产生触发器 // 向Scheduler中添加job任务和trigger触发器 sf.getScheduler().scheduleJob(jobDetail, trigger); // 启动 // sche.start();&#125;/** * 指定时间执行，一天执行一次 * * @param jobName任务名 * @param triggerName触发器名 * @param jobClass执行任务的类 * @param time指定时间 * 格式HH:mm如15:00指下午3点执行 * @throws SchedulerException * @throws ParseException */public static void addJob(String jobName, String triggerName, Class&lt;? extends Job&gt; jobClass, String time)throws SchedulerException, ParseException &#123; Calendar cal = Calendar.getInstance(); String nowTime = new SimpleDateFormat(&quot;yyyyMMdd&quot;).format(cal.getTime()); time = time.replace(&quot;:&quot;, &quot;&quot;); time = nowTime + time; Date startTime = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;).parse(time); // 创建一个SchedulerFactory工厂实例 getSchedulerFactory(); // 通过SchedulerFactory构建Scheduler对象 // Scheduler sche = sf.getScheduler(); // 用于描叙Job实现类及其他的一些静态信息，构建一个作业实例 JobDetail jobDetail = JobBuilder.newJob(jobClass).withIdentity(jobName, JOB_GROUP_NAME).build(); // 构建一个触发器，规定触发的规则 Trigger trigger = TriggerBuilder.newTrigger() // 创建一个新的TriggerBuilder来规范一个触发器 .withIdentity(triggerName, TRIGGER_GROUP_NAME) // 给触发器起一个名字和组名 .startAt(startTime) .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInHours(24).repeatForever()// 一直执行 ).build();// 产生触发器 // 向Scheduler中添加job任务和trigger触发器 sf.getScheduler().scheduleJob(jobDetail, trigger); // 启动 // sche.start();&#125;/** * 启动task * * @throws SchedulerException */public static void taskStart() throws SchedulerException &#123; if (sf == null) &#123; return; &#125; sf.getScheduler().start();&#125;/** * 改变每天定时跑的定时器的时间 如3点改为5点 * * @param triggerName * 原触发器名 * @param changetriggerName * 新任务触发器名 * @param time * 新时刻 * @throws SchedulerException * @throws ParseException */public static void changeTaskJob(String triggerName, String changetriggerName, String time)throws SchedulerException, ParseException &#123; if (sf == null) &#123; return; &#125; Calendar cal = Calendar.getInstance(); String nowTime = new SimpleDateFormat(&quot;yyyyMMdd&quot;).format(cal.getTime()); time = time.replace(&quot;:&quot;, &quot;&quot;); time = nowTime + time; Date startTime = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;).parse(time); // 构建一个触发器，规定触发的规则 Trigger trigger = TriggerBuilder.newTrigger() // 创建一个新的TriggerBuilder来规范一个触发器 .withIdentity(changetriggerName, TRIGGER_GROUP_NAME) // 给触发器起一个名字和组名 .startAt(startTime) .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInHours(24).repeatForever()// 一天一次 ).build();// 产生触发器 // 根据触发器获取指定的Job然后更改此Job的触发器 // 新的触发器不需要旧的触发器的名称相同 sf.getScheduler().rescheduleJob(new TriggerKey(triggerName, TRIGGER_GROUP_NAME)// 获取TriggerKey（用来标识唯一的Trigger） , trigger);&#125;/** * 移除一个任务 * * @param jobName * @param triggerName */public static void removeJob(String jobName, String triggerName) throws SchedulerException &#123; if (sf == null) &#123; return; &#125; TriggerKey triggerKey = TriggerKey.triggerKey(triggerName, TRIGGER_GROUP_NAME); sf.getScheduler().pauseTrigger(triggerKey);// 停止触发器 sf.getScheduler().unscheduleJob(triggerKey);// 移除触发器 sf.getScheduler().deleteJob(JobKey.jobKey(jobName, JOB_GROUP_NAME));// 删除任务&#125;&#125; 测试类task112345678910111213141516package com.hzzzy.task;import java.text.SimpleDateFormat;import org.quartz.Job;import org.quartz.JobExecutionContext;import org.quartz.JobExecutionException;public class TestTask1 implements Job &#123; @Override public void execute(JobExecutionContext context) throws JobExecutionException &#123; System.out.println(&quot;&gt;&gt;&gt;task1启动&gt;&gt;时间：&quot; + new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(context.getFireTime())); &#125;&#125; 测试类task2123456789101112131415package com.hzzzy.task;import java.text.SimpleDateFormat;import org.quartz.Job;import org.quartz.JobExecutionContext;import org.quartz.JobExecutionException;public class TestTask2 implements Job &#123; @Override public void execute(JobExecutionContext context) throws JobExecutionException &#123; System.out.println(&quot;&gt;&gt;&gt;task2启动&gt;&gt;时间：&quot; + new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(context.getFireTime())); &#125;&#125; 运行类12345678910111213141516171819202122232425262728package com.hzzzy.task;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import org.quartz.SchedulerException;public class Test &#123; public static void main(String[] args) &#123; try &#123; System.out.print(&quot;&gt;&gt;&gt;&gt;&gt;当前时间:&quot; + new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(new Date())); QuartzUtil.addJob(&quot;Test1&quot;, &quot;Test1&quot;, TestTask1.class, 70, 2); QuartzUtil.addJob(&quot;Test2&quot;, &quot;Test2&quot;, TestTask2.class, &quot;14:40&quot;); QuartzUtil.taskStart(); Thread.sleep(60 * 1000); QuartzUtil.changeTaskJob(&quot;Test2&quot;, &quot;Test2&quot;, &quot;14:42&quot;); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; catch (SchedulerException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行Test，得到一下结果：12345678&gt;&gt;&gt;&gt;&gt;当前时间:2017-12-04 14:39:32&gt;&gt;&gt;task1启动&gt;&gt;时间：2017-12-04 14:39:34&gt;&gt;&gt;task2启动&gt;&gt;时间：2017-12-04 14:40:00&gt;&gt;&gt;task1启动&gt;&gt;时间：2017-12-04 14:40:44&gt;&gt;&gt;task1启动&gt;&gt;时间：2017-12-04 14:41:54&gt;&gt;&gt;task2启动&gt;&gt;时间：2017-12-04 14:42:00&gt;&gt;&gt;task1启动&gt;&gt;时间：2017-12-04 14:43:04...","tags":[{"name":"java","slug":"java","permalink":"www.hzzzy.top/tags/java/"}]},{"title":"利用maven-profile实现配置分离多环境切换","date":"2017-11-28T07:39:57.000Z","path":"2017/11/28/maven-profile/","text":"java程序员，日常中，常混在开发环境、测试环境、生成环境，而这些环境的配置与项目需求往往也不一样。手动配置，容易出错或混淆，maven-profile的出现可解决这些问题，从此不怕配置写错了，挨骂了。 maven-profile 的使用如下: 1. 配置profile编辑项目的pom.xml文件，在project节点添加profile配置：123456789101112131415161718192021222324252627&lt;profiles&gt; &lt;profile&gt; &lt;!-- 开发环境 --&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;profiles.active&gt;dev&lt;/profiles.active&gt; &lt;/properties&gt; &lt;activation&gt; &lt;!-- 设置默认激活这个配置 --&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;!-- 生产环境 --&gt; &lt;id&gt;prod&lt;/id&gt; &lt;properties&gt; &lt;profiles.active&gt;prod&lt;/profiles.active&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;!-- 测试环境 --&gt; &lt;id&gt;test&lt;/id&gt; &lt;properties&gt; &lt;profiles.active&gt;test&lt;/profiles.active&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 2. 增加配置文件针对不同的环境，配置不同的属性文件，本次共配置开发、测试、生产3种配置文件，如： 3.maven资源插件配置编辑项目的pom.xml文件，在build节点配置资源文件的位置，如：123456789101112131415161718192021 &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;!-- 资源根目录排除各环境的配置，使用单独的资源目录来指定 --&gt; &lt;excludes&gt; &lt;exclude&gt;dev/*&lt;/exclude&gt; &lt;exclude&gt;test/*&lt;/exclude&gt; &lt;exclude&gt;prod/*&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources/$&#123;profiles.active&#125;&lt;/directory&gt; &lt;/resource&gt;&lt;/resources&gt; 4.激活profile 默认激活profile打包时若没有指定profile，则根据profile配置中的默认激活profile，如： true 使用-P参数显式激活profile在maven操作时使用-P参数显式指定需要激活的profile，如：mvn package –Pdev或者在eclipse打包时指定，如：","tags":[{"name":"maven","slug":"maven","permalink":"www.hzzzy.top/tags/maven/"}]},{"title":"Kafka技术原理","date":"2017-11-28T03:02:16.000Z","path":"2017/11/28/kafka/","text":"Kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率的特点。官网地址：http://kafka.apache.org 一、整体架构 1、消息的发布（publish）简称producer，消息的订阅（subscribe）称为consumer，中间的存储阵列称为broker。2、多个broker协同合作，producer、broker和consumer三者通过zookeeper来协调请求和转发。3、producer生产和推送（push）到broker，consumer从broker拉取（pull）数据并行处理。4、broker端不维护数据的消费状态，提高broker的TPS；5、直接使用磁盘进行存储，线性读写，速度快，避免了数据在JVM内存和系统内存之间的复制，减少耗性能的创建对象和垃圾回收。6、Kafka使用scala编写，可以运行在JVM上。 二、术语Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker。一台Kafka服务器就是一个broker，一个broker可以容纳多个topic。Topic:每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上，但是用户只需要指定消息的Topic即可生产或消费数据不必关心数据存于何处。Partition:Partition是物理上的概念，每个Topic包含一个或多个Partition。Producer:负责发布消息到Kafka broker。Consumer:消息消费者，向Kafka broker读取消息的客户端。Consumer Group:每个consumer属于一个特定的Consumer Group（可为每个consumer指定group name，若不指定group name则属于默认的group）。各个consumer可以组成一个组，每个消息只能被组中的一个consumer消费，如果一个消息可以被多个consumer消费的话，那么这些consumer必须在不同的组。Offset:Kafka的存储文件都是按照offset.kafka来命名，好处是方便查找。Replica:partition的副本，保障partition的高可用。Leader:replica中的一个角色，producer和consumer只能跟leader交互。Follower:replica中的一个角色，从leader中复制数据。 三、技术原理1、Topics/logs 一个topic可认为是一类消息，每个topic将被分成多个partition（分区），每个partition在存储层面是append log文件。任何发布到此partition的消息都会被直接追加到log文件的尾部，每条消息在文件中的位置称为offset（偏移量），offset为一个long型数字，它是唯一标识一条消息。Kafka并没有提供其它额外的索引机制来存储offset，因为Kafka不允许对消息进行“随机读写”。 Kafka和JMS（Java Message Service）都实现了activeMQ，不同的是：即使消息被消费，消息仍然不会被立即删除，日志文件将会根据broker中的配置要求，保留一定的时间之后删除。 对于consumer而言，它需要保存消费消息的offset，对于offset的保存和使用由consumer来控制；当consumer正常消费消息时，offset将会线性向前驱动，即消息将依次顺序被消费，事实上consumer可以使用任意顺序消费消息，只需要将offset重置为任何值。 Kafka集群不维护任何consumer和producer状态信息，这些信息由Zookeeper保存，因此producer和consumer的实现是非常轻量级，它们可以随意离开，而不会对集群造成额外的影响。 partition的设计目的有多个，最根本原因是Kafka基于文件存储，通过分区可以将日志内容分散到多个server上，来避免文件尺寸达到单机磁盘的上限，每个partition都会被当前server（Kafka实例）保存，可以将一个topic切分任意多个partitions来提高消息保存/消费的效率，此外越多的partitions意味着可以容纳更多的consumer，有效提升并发消费的能力。 2、Distribution 一个Topic的多个partitions被分布在Kafka集群中的多个server上，每个server（Kafka实例）负责partitions中消息的读写操作，此外Kafka还可以配置partitions需要备份的个数（replica），每个partition将会备份到多台机器上以提高可用性。 基于replicated方案，那么就意味着需要对多个备份进行调度，每个partition都有一个server为“leader”，leader负责所有的读写操作，如果leader失败，那么将会有其它follower来接管成为新的leader，follower只是单调的和leader跟进，同步消息即可，由此可见，作为leader的server承载了全部的请求压力，从集群的整体考虑，有多少个partitions就意味着有多少个“leader”，Kafka会将“leader”均衡分散在每个实例上，来确保整体的性能稳定。 3、Producers producer将消息发布到指定的Topic中，同时producer也能指定将此消息归属于那个partition。 4、Consumers 本质上Kafka只支持Topic，每个consumer属于一个consumer group，反过来说，每个group中可以有多个consumer，发送到topic的消息，只会被订阅到此topic的每个group的一个consumer消费。 如果所有的consumer都具有相同的group，这种情况和queue模式很像，消息将会在consumers之间负载均衡。如果所有的consumer都具有不同的group，那这就是“发布”-“订阅”，消息将会广播给所有的消费者。 在Kafka中，一个partition中的消息只会被group中的一个consumer消费，每个group中consumer消息消费相互独立，可以认为一个“group”是一个订阅者，一个topic中的每个partitions只会被一个“订阅者”中的一个consumer消费，不过一个consumer可以消费多个partitions中的数据。Kafka只能保证一个partition中的消息被某个consumer消费时，消息是顺序的，事实上，从Topic角度来看，消息仍不是有序的。Kafka的设计原理决定，对于一个topic，同一个group中不能有多于partitions个数的consumer同时消费，否则将意味着某些consumer将无法得到消息。 5、消息传送机制 1）at most once：最多一次，发送一次，无论成败，将不会重发。消费者fetch消息，先保存offset，然后处理消息，当client保存offset后，但是在消息处理过程中出现异常导致部分消息未能继续处理，那么此后“未处理”的消息将不能被fetch到。 2）at least once：消息至少发送一次，如果消息未能接受成功，可能会重发，直到接收成功。消费者fetch消息，先处理消息，然后保存offset，如果消息处理成功，但是在保存offset阶段由于Zookeeper异常导致保存操作未能执行成功，这就导致接下来再次fetch时可能获得上次已经处理过的消息，原因offset没有及时的提交给Zookeeper，Zookeeper恢复正常还是之前的offset状态。 3）exactly once：消息只会发送一次。Kafka中并没有严格地去实现。 四、参数配置说明 五、性能调优1、设置vm.swappiness为较小值（swap机制） 计算机的内存分为虚拟内存和物理内存，物理内存是真实的内存，虚拟内存是用磁盘来代替内存，通过swap机制实现磁盘到物理内存的加载和替换。在写文件的时候，Linux首先将数据写入没有被使用的内存中，这些内存被称为内存页（page cache），然后读的时候，Linux会优先从page cache中查找，如果找不到就会从磁盘中查找。当物理内存使用达到一定的比例后，Linux就会进行swap，使用磁盘作为虚拟内存。通过cat /proc/sys/vm/swappiness可以看到swap参数，这个参数表示swap磁盘代替的虚拟内存占了多少百分比。0表示最大限度的使用内存，100表示使用swap磁盘。系统默认的参数60，当物理内存使用率达到40%，就会频繁进行swap，影响系统性能，推荐将vm.swappiness设置为较低值1。 2、提高partition个数 Kafka是一个高吞吐量分布式消息系统，其高性能有两个特点：1）利用磁盘连续读写性高于随机读写；2）并发，将一个topic拆分多个partition。因此，将一个topic拆分为多个partition可以提高吞吐量，但有个前提就是partition需要位于不同磁盘，如果多个partition位于同一个磁盘，那么意味着有多个进程同时对一个磁盘的多个文件进行读写，使得操作系统会对磁盘读写进行频繁调度，也就是破坏磁盘读写的连续性。 3、挂盘优于磁盘阵列，小块盘优于大块盘 充分利用多磁盘并发读写，又能保证每个磁盘连续读写的特性。 4、JVM参数优化 推荐使用最新的G1来代替CMS作为垃圾回收器。G1是一种适用于服务器端的垃圾回收器，很好地平衡了吞吐量和响应能力。 5、Producer优化 提高message的缓冲区大小；默认发送不进行压缩，推荐配置一种适合的压缩算法； 6、Broker参数优化 1）网络和IO操作线程配置优化 broker处理消息的最大线程数 num.network.threads=xxx broker处理磁盘IO的线程数 num.io.threads=xxx num.network.threads用于接收并处理网络请求的线程数，默认为3。其内部实现是采用selector模型，启动一个线程作为acceptor来负责建立连接，再配合启动num.network.threads个线程来轮流负责从socket读取请求，一般无需改动，除非上下游并发请求量过大。一般num.network.threads主要处理网络io，读写缓冲区数据，基本没有io等待，配置线程数量为CPU核数加1。num.io.threads主要进行磁盘io操作，高峰期可能有些io等待，因此配置需要大些，配置线程数量为CPU核数2倍，最大不超过3倍。 2）log数据文件刷盘策略 为了提高producer写入吞吐量，需要定期批量写文件。 log.flush.interval.messages=10000 log.flush.interval.ms=1000 3）日志保存策略 根据实际应用及数据增长速率调整磁盘数据过期时间。 log.retention.hours=72 log.segment.bytes=1073741824 4）配置jmx服务 Kafka server中默认是不启动jmx端口的，需要用户配置。 vim bin/kafka-run-class.sh 最前面添加一行 JMX_PORT=8060 5）Replica相关配置 replica.lag.time.max.ms=10000 replica.lag.max.messages=4000 num.replica.fetchers=1 default.replication.factor=1 在Replica上会启动若干Fetch线程把对应的数据同步到本地，而num.replica.fetchers这个参数是用来控制Fetch线程的数量；Replica过少会影响数据的可用性，太多则会白白浪费存储资源，一般建议在2~3为宜。 6）分区数量 num.partitions=1 分区数影响topic并发读写的线程数，通常设置为磁盘数来提高Kafka的吞吐量。","tags":[{"name":"kafka","slug":"kafka","permalink":"www.hzzzy.top/tags/kafka/"}]},{"title":"ExecutorService使用","date":"2017-11-27T03:27:44.000Z","path":"2017/11/27/ExecutorService/","text":"java.util.concurrent.ExecutorService是Java中对线程池定义的一个接口，ExecutorService继承Executor。 Executor：一个接口，其定义了一个接收Runnable对象的方法executor，其方法签名为executor(Runnable command) ExecutorService：是一个比Executor使用更广泛的子类接口，其提供了生命周期管理的方法，以及可跟踪一个或多个异步任务执行状况返回Future的方法。 事实上，在java.util.concurrent 包中的 ExecutorService 的实现就是一个线程池的实现。 前言一般情况下，多线程的实现或者异步执行任务是通过继承Thread类或Runnable接口实现。但这种方式缺点很明显： 线程缺乏统一管理，可能无限制新建线程，相互之间竞争。 性能差。 容易造成死机，资源耗尽。 然而，使用线程池的好处则不言而喻了。 其支持多种不同类型的任务执行策略。 提供对生命周期的支持，以及统计信息收集，应用程序管理机制和性能监视等机制。 提供定时执行、定期执行、单线程、并发数控制等功能。 有效控制最大并发线程数，提高系统资源的使用率。 最简单的ExecutorService样例1234567ExecutorService executorService = Executors.newFixedThreadPool(5); executorService.execute(new Runnable() &#123; public void run() &#123; System.out.println(&quot;Asynchronous task&quot;); &#125; &#125;); executorService.shutdown(); 上述代码使用newFixedThreadPool() 工厂方法创建一个 ExecutorService，创建了容纳5个线程的线程池。然后，向 execute() 方法中传一个异步的 Runnable 接口的实现，这样做会让 ExecutorService 线程池中的某个线程执行这个传递进来的 Runnable 线程。你会发现ExecutorService是通过Executors创建的，那Executors到底是什么鬼来的呢？ Executors：提供了一系列静态工厂方法用于创建各种线程池。 此外Executors一共可以创建下面这四类线程池： newCachedThreadPool 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 ExecutorService的执行ExecutorService有如下几个执行方法：12345- execute(Runnable)- submit(Runnable)- submit(Callable)- invokeAny(...)- invokeAll(...) 1. execute(Runnable)方法 execute(Runnable) 接收一个 java.lang.Runnable 对象作为参数，并且以异步的方式执行它。实例如下：1234567ExecutorService executorService = Executors.newSingleThreadExecutor(); executorService.execute(new Runnable() &#123; public void run() &#123; System.out.println(&quot;Asynchronous task&quot;); &#125; &#125;); executorService.shutdown() 使用这种方式没有办法获取执行Runnable之后的结果，如果你希望获取运行之后的返回值，就必须使用 接收Callable参数的 execute()方法，后者将会在下文中提到。 2.submit(Runnable)方法 submit(Runnable) 同样接收一个Runnable的实现作为参数，但是会返回一个Future对象。这个 Future 对象可以用于判断Runnable是否结束执行。实例如下：1234567Future future = executorService.submit(new Runnable() &#123; public void run() &#123; System.out.println(&quot;Asynchronous task&quot;); &#125; &#125;); //如果任务结束执行则返回 null System.out.println(&quot;future.get()=&quot; + future.get()); 3.submit(Callable)方法submit(Callable)和方法submit(Runnable)比较类似，但是区别则在于它们接收不同的参数类型。Callable的实例与Runnable的实例很类似，但是Callable的call()方法可以返回一个结果。方法Runnable.run() 则不能返回结果。Callable的返回值可以从方法submit(Callable)返回的 Future 对象中获取实例如下：1234567Future future = executorService.submit(new Callable()&#123; public Object call() throws Exception &#123; System.out.println(&quot;Asynchronous Callable&quot;); return &quot;Callable Result&quot;; &#125; &#125;); System.out.println(&quot;future.get() = &quot; + future.get()); 4.inVokeAny()方法invokeAny()接收一个包含Callable对象的集合作为参数。调用该方法不会返回Future对象，而是返回集合中某一个Callable对象的结果，而且无法保证调用之后返回的结果是哪一个Callable，只知道它是这些 Callable中一个执行结束的 Callable 对象。如果一个任务运行完毕或者抛出异常，方法会取消其它的 Callable 的执行。实例如下：1234567891011121314151617181920ExecutorService executorService = Executors.newSingleThreadExecutor(); Set&lt;Callable&lt;String&gt;&gt; callables = new HashSet&lt;Callable&lt;String&gt;&gt;(); callables.add(new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; return &quot;Task 1&quot;; &#125; &#125;); callables.add(new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; return &quot;Task 2&quot;; &#125; &#125;); callables.add(new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; return &quot;Task 3&quot;; &#125; &#125;); String result = executorService.invokeAny(callables); System.out.println(&quot;result = &quot; + result); executorService.shutdown(); 5.invokeAll()方法invokeAll()会调用存在于参数集合中的所有Callable对象，并且返回一个包含Future对象的集合，你可以通过这个返回的集合来管理每个Callable的执行结果。需要注意的是，任务有可能因为异常而导致运行结束，所以它可能并不是真的成功运行了。但是我们没有办法通过Future对象来了解到这个差异。实例如下：1234567891011121314151617181920ExecutorService executorService = Executors.newSingleThreadExecutor(); Set&lt;Callable&lt;String&gt;&gt; callables = new HashSet&lt;Callable&lt;String&gt;&gt;(); callables.add(new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; return &quot;Task 1&quot;; &#125; &#125;); callables.add(new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; return &quot;Task 2&quot;; &#125; &#125;); callables.add(new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; return &quot;Task 3&quot;; &#125; &#125;); String result = executorService.invokeAny(callables); System.out.println(&quot;result = &quot; + result); executorService.shutdown(); ExecutorService的关闭当我们使用完成ExecutorService之后应该关闭它，否则它里面的线程会一直处于运行状态。ExecutorService的关闭，可以调用ExecutorService.shutdown()方法。在调用shutdown()方法之后，ExecutorService不会立即关闭，但是它不再接收新的任务，直到当前所有线程执行完成才会关闭，所有在shutdown()执行之前提交的任务都会被执行。如果想立即关闭ExecutorService，可以调用ExecutorService.shutdownNow()方法。这个动作将跳过所有正在执行的任务和被提交还没有执行的任务。但是它并不对正在执行的任务做任何保证，有可能它们都会停止，也有可能执行完成。 等待ExecutorService的结束由于调用shutdown方法，ExecutorService不是立即关闭，那么，怎么知道ExecutorService的线程池里的线程都执行完毕了呢？可以使用awaitTermination方法检查，例子如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849ExecutorService threadPool = Executors.newCachedThreadPool();threadPool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Asynchronous task1&quot;); &#125;&#125;);threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(20 * 1000); // 休眠20秒 &#125; catch (InterruptedException e) &#123; try &#123; Thread.sleep(60 * 1000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; System.out.println(&quot;Asynchronous task2&quot;); &#125;&#125;);threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(4 * 1000); // 休眠4秒 &#125; catch (InterruptedException e) &#123; try &#123; Thread.sleep(60 * 1000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; System.out.println(&quot;Asynchronous task3&quot;); &#125;&#125;);// 启动一次顺序关闭，执行以前提交的任务，但不接受新任务。threadPool.shutdown();try &#123; // 请求关闭、发生超时或者当前线程中断，无论哪一个首先发生之后，都将导致阻塞，直到所有任务完成执行 // 设置最长等待10秒 while (!threadPool.awaitTermination(10, TimeUnit.SECONDS)) &#123; System.out.println(&quot;&gt;&gt;&gt;&gt;线程尚未结束&gt;&gt;&gt;&gt;&quot;); &#125; System.out.println(&quot;&gt;&gt;&gt;&gt;线程已结束!&gt;&gt;&gt;&gt;&quot;);&#125; catch (InterruptedException e) &#123; System.out.println(&quot;异常:&quot; + e.getMessage());&#125; 结果如下：12345Asynchronous task1Asynchronous task3&gt;&gt;&gt;&gt;线程尚未结束&gt;&gt;&gt;&gt;Asynchronous task2&gt;&gt;&gt;&gt;线程已结束!&gt;&gt;&gt;&gt;","tags":[{"name":"java","slug":"java","permalink":"www.hzzzy.top/tags/java/"}]},{"title":"maven生产中的常用命令","date":"2017-11-26T02:51:14.000Z","path":"2017/11/26/maven-order/","text":"Maven项目对象模型(POM)，可以通过一小段描述信息来管理项目的构建，报告和文档的软件项目管理工具。Maven 除了以程序构建能力为特色之外，还提供高级项目管理工具。 maven 依赖库 http://repo2.maven.org/maven2/ http://maven.aliyun.com/nexus/content/groups/public/ http://maven.oschina.net/content/groups/public/maven jar包查询（可下载） http://mvnrepository.com/ 常用命令1. 创建Maven的普通Java项目123mvn archetype:create -DgroupId=packageName -DartifactId=projectName 2. 创建Maven的Web项目1234mvn archetype:create -DgroupId=packageName -DartifactId=webappName -DarchetypeArtifactId=maven-archetype-webapp 3. 安装依赖包到本地Repository以oracle包为例1mvn install:install-file -DgroupId=com.Oracle -DartifactId=ojdbc14 -Dversion=10.2.0.2.0 -Dpackaging=jar -Dfile=E:\\oracle\\ojdbc14-10.2.0.2.0.jar 4. 安装包到私服以netty-all-4.0.23.Final为例1mvn deploy:deploy-file -Dfile=F:\\lib\\repository\\io\\netty\\netty-all\\4.0.23.Final\\netty-all-4.0.23.Final.jar -DgroupId=io.netty -DartifactId=netty-all -Dversion=4.0.23.Final -Durl=http://10.250.11.89:8081/nexus/content/groups/public/ -Dpackaging=jar -DrepositoryId=nexus-snapshots 5. 编译源代码1mvn compile 6. 编译测试代码1mvn test-compile 7. 打包1mvn package 8. 清除产生的项目1mvn clean 9. 只打jar包1mvn jar:jar 10. 运行项目于jetty上1mvn jetty:run","tags":[{"name":"maven","slug":"maven","permalink":"www.hzzzy.top/tags/maven/"}]},{"title":"GenericKeyedObjectPool的应用demo","date":"2017-11-25T09:25:11.000Z","path":"2017/11/25/GenericKeyedObjectPool/","text":"Commons-pool是一个apache开源组织下的众多项目的一个。其被广泛地整合到众多需要对象池功能的项目中。本文是commons-pool2的一个带key的池（GenericKeyedObjectPool）简单应用。 原理不多说，具体Commons-pool源码分析可参考《Apache Commons-Pool 源码分析》拿起键盘就开撸。。。 新建maven工程，并添加以下依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt;&lt;/dependency&gt; 通用业务对象生成Factory CommonWorkFactory类实现的是KeyedPoolableObjectFactory接口，需实现以下方法。其中makeObject在创建实例时调用，通过自定义代码生成并返回需求的实例，注意要以一个形参为key创建；activateObject方法在从Pool中拿出一个实例的同时调用，passivateObject方法在实例返还给Pool时调用。validateObject方法是验证该实例是否安全。 这里（key-实例）的实现对应的是： Key0-com.hzzzy.test.handle.Key0WorkHandle Key1-com.hzzzy.test.handle.Key1WorkHandle Key2-com.hzzzy.test.handle.Key2WorkHandle 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.hzzzy.test.factory;import org.apache.commons.pool2.KeyedPooledObjectFactory;import org.apache.commons.pool2.PooledObject;import org.apache.commons.pool2.impl.DefaultPooledObject;import com.hzzzy.test.handle.ObjectWorkHandle;public class CommonWorkFactory implements KeyedPooledObjectFactory&lt;String, ObjectWorkHandle&gt; &#123; /** * 重新初始化实例返回池 */ @Override public void activateObject(String arg0, PooledObject&lt;ObjectWorkHandle&gt; arg1) throws Exception &#123; arg1.getObject().setActive(true); &#125; /** * 销毁被破坏的实例 */ @Override public void destroyObject(String arg0, PooledObject&lt;ObjectWorkHandle&gt; arg1) throws Exception &#123; arg1 = null; &#125; /** * 创建一个实例到对象池 */ @Override public PooledObject&lt;ObjectWorkHandle&gt; makeObject(String arg0) throws Exception &#123; Class&lt;?&gt; c = null; StringBuffer classStr = new StringBuffer(); classStr.append(&quot;com.hzzzy.test.handle.&quot;); classStr.append(arg0 + &quot;WorkHandle&quot;); c = Class.forName(classStr.toString()); ObjectWorkHandle handle = (ObjectWorkHandle) c.newInstance(); return new DefaultPooledObject&lt;ObjectWorkHandle&gt;(handle); &#125; /** * 取消初始化实例返回到空闲对象池 */ @Override public void passivateObject(String arg0, PooledObject&lt;ObjectWorkHandle&gt; arg1) throws Exception &#123; arg1.getObject().setActive(false); &#125; /** * 验证该实例是否安全 true:正在使用 */ @Override public boolean validateObject(String arg0, PooledObject&lt;ObjectWorkHandle&gt; arg1) &#123; // 这里可以判断实例状态是否可用 if (arg1.getObject().isActive()) return true; else return false; &#125;&#125; KeyPoolFactory对象池工厂 定义对象池工厂，GenericKeyedObjectPoolConfig设置对象池的参数。 Pool中实例的操作，主要包括实例的获取及实例的返还。使用完实例之后，需及时返还，否则实例还是处理激活状态，不能为被后面的申请重复利用。GenericKeyedObjectPool是带key的pool的操作，相应的方法也带key了。getKey方法纯碎是测试用的，用来生成相应的与业务实例的key。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.hzzzy.test.factory;import org.apache.commons.pool2.impl.GenericKeyedObjectPool;import org.apache.commons.pool2.impl.GenericKeyedObjectPoolConfig;import com.hzzzy.test.handle.ObjectWorkHandle;public class KeyPoolFactory &#123; /** * 对象池 */ private static GenericKeyedObjectPool&lt;String, ObjectWorkHandle&gt; pool; /** * 对象池的参数设置 */ private static final GenericKeyedObjectPoolConfig config; /** * 对象池每个key最大实例化对象数 */ private final static int TOTAL_PERKEY = 400; /** * 对象池每个key最大的闲置对象数 */ private final static int IDLE_PERKEY = 4; static &#123; config = new GenericKeyedObjectPoolConfig(); config.setMaxTotalPerKey(TOTAL_PERKEY); config.setMaxIdlePerKey(IDLE_PERKEY); /** 支持jmx管理扩展 */ config.setJmxEnabled(true); config.setJmxNamePrefix(&quot;myPoolProtocol&quot;); /** 保证获取有效的池对象 */ config.setTestOnBorrow(true); config.setTestOnReturn(true); &#125; /** * 初始化对象池 */ private synchronized static void init() &#123; if (pool != null) return; pool = new GenericKeyedObjectPool&lt;String, ObjectWorkHandle&gt;(new CommonWorkFactory(), config); &#125; /** * 从对象池中获取对象 * * @param key * @return * @throws Exception */ public static ObjectWorkHandle getWorkHandle(String key) throws Exception &#123; if (pool == null) &#123; init(); &#125; return pool.borrowObject(key); &#125; /** * 归还对象 * * @param key * @param bean */ public static void returnBean(String key, ObjectWorkHandle bean) &#123; if (pool == null) &#123; init(); &#125; pool.returnObject(key, bean); &#125; /** * 关闭对象池 */ public synchronized static void close() &#123; if (pool != null &amp;&amp; !pool.isClosed()) &#123; pool.close(); pool = null; &#125; &#125; /** * 当前池里的处于闲置状态的实例 */ public static int getNumIdle() &#123; if (pool == null) &#123; init(); &#125; return pool.getNumIdle(); &#125; /** * 池中所有在用实例 */ public static int getNumActive() &#123; if (pool == null) &#123; init(); &#125; return pool.getNumIdle(); &#125; /** * 获取对象对象的key * * @param i * @return */ public static String getKey(int i) &#123; // 这里就产生3个key，分别是Key0，Key1，Key2 i = i % 3; return &quot;Key&quot; + i; &#125;&#125; 通用业务接口 根据业务场景的抽象类 1234567891011121314151617181920212223242526package com.hzzzy.test.handle;/** * 通用业务接口 * */public abstract class ObjectWorkHandle &#123; private boolean active; public ObjectWorkHandle() &#123; active = true; &#125; /** * 测试用的调用方法 * * @return */ public abstract void handleWork(); public boolean isActive() &#123; return active; &#125; public void setActive(boolean active) &#123; this.active = active; &#125;&#125; key0对应的业务类 具体业务的实例 123456789package com.hzzzy.test.handle;public class Key0WorkHandle extends ObjectWorkHandle &#123; @Override public void handleWork() &#123; System.out.println(&quot;当前是Key0在执行&quot;); &#125;&#125; key1对应的业务类123456789package com.hzzzy.test.handle;public class Key1WorkHandle extends ObjectWorkHandle &#123; @Override public void handleWork() &#123; System.out.println(&quot;当前是Key1在执行&quot;); &#125;&#125; key2对应的业务类123456789package com.hzzzy.test.handle;public class Key2WorkHandle extends ObjectWorkHandle &#123; @Override public void handleWork() &#123; System.out.println(&quot;当前是Key2在执行&quot;); &#125;&#125; 最后测试类123456789101112131415161718192021222324252627282930package com.hzzzy.test;import com.hzzzy.test.factory.KeyPoolFactory;import com.hzzzy.test.handle.ObjectWorkHandle;public class PoolRunTest &#123; public static void main(String[] args) &#123; try &#123; // 当前池里的实例数量 System.out.println(&quot;池中所有在用实例pool.getNumActive()：&quot; + KeyPoolFactory.getNumActive()); // 当前池里的处于闲置状态的实例 System.out.println(&quot;池中处于闲置状态的实例pool.getNumIdle()：&quot; + KeyPoolFactory.getNumIdle()); for (int i = 0; i &lt; 9; i++) &#123; String key = KeyPoolFactory.getKey(i); ObjectWorkHandle handle = KeyPoolFactory.getWorkHandle(key); handle.handleWork(); // 用完后return回去 KeyPoolFactory.returnBean(key, handle); &#125; // 当前池里的实例数量 System.out.println(&quot;池中所有在用实例pool.getNumActive()：&quot; + KeyPoolFactory.getNumActive()); // 当前池里的处于闲置状态的实例 System.out.println(&quot;池中处于闲置状态的实例pool.getNumIdle()：&quot; + KeyPoolFactory.getNumIdle()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 以上代码执行的结果：12345678910111213池中所有在用实例：0池中处于闲置状态的实例：0当前是Key0在执行当前是Key1在执行当前是Key2在执行当前是Key0在执行当前是Key1在执行当前是Key2在执行当前是Key0在执行当前是Key1在执行当前是Key2在执行池中所有在用实例：3池中处于闲置状态的实例：3 循环9次，3次key相同的，调用的是对象池里面对应的相同的实例。此时pool里面只有3个实例，3个不同的实例，与Key0，Key1，Key2相对应的实例。","tags":[{"name":"java","slug":"java","permalink":"www.hzzzy.top/tags/java/"}]},{"title":"java类的初始化顺序","date":"2017-11-23T06:42:21.000Z","path":"2017/11/23/java-init-order/","text":"1、对于静态变量、静态初始化块、变量、初始化块、构造器，他们的初始化顺序是怎样的呢？ 下面以代码测试： 1234567891011121314151617181920212223public class JavaInitOrder &#123; // 静态变量 public static String staticVar = &quot;静态变量&quot;; // 变量 public String var = &quot;变量&quot;; // 静态初始化块 static &#123; System.out.println(staticVar); System.out.println(&quot;静态初始化块&quot;); &#125; // 初始化块 &#123; System.out.println(var); System.out.println(&quot;初始化块&quot;); &#125; // 构造器 public JavaInitOrder() &#123; System.out.println(&quot;构造器&quot;); &#125; public static void main(String[] args) &#123; new JavaInitOrder(); &#125;&#125; 程序启动后，输出的结果是：12345静态变量静态初始化块变量初始化块构造器 由此可得出他们的初始化顺序:（静态变量、静态初始化块）&gt;（变量、初始化块）&gt;构造器 如果是继承关系的呢？ 2.继承关系中，静态变量、静态初始化块、变量、初始化块、构造器的初始化顺序又是怎样的呢？ 做以下测试，写一个继承JavaInitOrder的子类JavaInitOrderSon： 1234567891011121314151617181920212223public class JavaInitOrderSon extends JavaInitOrder &#123; // 静态变量 public static String s_staticVar = &quot;子类-静态变量&quot;; // 变量 public String s_var = &quot;子类-变量&quot;; // 静态初始化块 static &#123; System.out.println(s_staticVar); System.out.println(&quot;子类-静态初始化块&quot;); &#125; // 初始化块 &#123; System.out.println(s_var); System.out.println(&quot;子类-初始化块&quot;); &#125; // 构造器 public JavaInitOrderSon() &#123; System.out.println(&quot;子类-构造器&quot;); &#125; public static void main(String[] args) &#123; new JavaInitOrderSon(); &#125;&#125; 启动JavaInitOrderSon类，输出的结果是:12345678910静态变量静态初始化块子类-静态变量子类-静态初始化块变量初始化块构造器子类-变量子类-初始化块子类-构造器 由此可得出继承关系的他们的初始化顺序:（父类-静态变量、静态初始化块）&gt;（子类类-静态变量、静态初始化块）&gt;（父类-变量、初始化块）&gt;父类-构造器&gt;（子类-变量、初始化块）&gt;子类构造器 继承关系中，有一点需要注意的是：并不是父类完全初始化完毕后才进行子类的初始化，实际上子类的静态变量和静态初始化块的初始化是在父类的变量、初始化块和构造器初始化之前就完成了。 3、静态变量和静态初始化块之间先后顺序又是怎样呢？ 上面两个例子还没完全区分开来，现做一下代码测试： 123456789101112131415161718192021222324public class StaticTest &#123; // 静态变量 public static Test1 staticVar1 = new Test1(); // 静态初始化块 static &#123; System.out.println(&quot;静态初始化块&quot;); &#125; // 静态变量 public static Test2 staticVar2 = new Test2(); public static void main(String[] args) &#123; new StaticTest(); &#125; public static class Test1 &#123; public Test1() &#123; System.out.println(&quot;静态变量--1--&quot;); &#125; &#125; public static class Test2 &#123; public Test2() &#123; System.out.println(&quot;静态变量--2--&quot;); &#125; &#125;&#125; 执行StaticTest类，得出的结果是：123静态变量--1--静态初始化块静态变量--2-- 如果我调换静态变量与静态初始化块的代码顺序呢：123456789101112131415161718192021222324public class StaticTest &#123; // 静态初始化块 static &#123; System.out.println(&quot;静态初始化块&quot;); &#125; // 静态变量 public static Test1 staticVar1 = new Test1(); // 静态变量 public static Test2 staticVar2 = new Test2(); public static void main(String[] args) &#123; new StaticTest(); &#125; public static class Test1 &#123; public Test1() &#123; System.out.println(&quot;静态变量--1--&quot;); &#125; &#125; public static class Test2 &#123; public Test2() &#123; System.out.println(&quot;静态变量--2--&quot;); &#125; &#125;&#125; 执行后，得出的结果是：123静态初始化块静态变量--1--静态变量--2-- 由此可得出结果：静态变量、静态初始化块的初始化顺序取决于他们的在类中出现的顺序。 （静态变量、静态初始化块）与（变量和初始化块）有雷同之处。 4.变量和初始化块之间的初始化顺序呢？ 稍微改动一下上面的代码就ok了：123456789101112131415161718192021222324public class StaticTest &#123; // 变量 public Test1 staticVar1 = new Test1(); // 初始化块 &#123; System.out.println(&quot;初始化块&quot;); &#125; // 变量 public Test2 staticVar2 = new Test2(); public static void main(String[] args) &#123; new StaticTest(); &#125; public static class Test1 &#123; public Test1() &#123; System.out.println(&quot;变量--1--&quot;); &#125; &#125; public static class Test2 &#123; public Test2() &#123; System.out.println(&quot;变量--2--&quot;); &#125; &#125;&#125; 执行得出的结果是：123变量--1--初始化块变量--2-- 调换一下变量和初始化块的位置顺序：12345678910111213141516171819202122232425public class StaticTest &#123; // 初始化块 &#123; System.out.println(&quot;初始化块&quot;); &#125; // 变量 public Test1 staticVar1 = new Test1(); // 变量 public Test2 staticVar2 = new Test2(); public static void main(String[] args) &#123; new StaticTest(); &#125; public static class Test1 &#123; public Test1() &#123; System.out.println(&quot;变量--1--&quot;); &#125; &#125; public static class Test2 &#123; public Test2() &#123; System.out.println(&quot;变量--2--&quot;); &#125; &#125;&#125; 执行后的出的结果是：123初始化块变量--1--变量--2-- 由此可得出一下结论：变量、初始化块的初始化顺序也是取决于他们的在类中出现的顺序。","tags":[{"name":"java","slug":"java","permalink":"www.hzzzy.top/tags/java/"}]},{"title":"eclipse修改dynamic web module version的问题","date":"2017-11-22T07:56:35.000Z","path":"2017/11/22/eclipse-dynamic/","text":"偶尔碰到这一类的问题，从svn上拉下来的maven管理的web工程，导入eclipse后，转换web工程的时候，出现Cannot change version of project facet Dynamic Web Module to 3.0 按照maven web工程转eclipse web工程的套路，在Project Facets里面中选择Dynamic Web Module，选择3.0版本的时候，报错，无法继续，然而在其他同事那里却没有这个问题，诚然不知什么造成的，最后默认是eclipse的错。不了了之之后，采用暴力方式修改： 找到工程项目的硬盘目录，打开项目下的.setting文件夹： 找到.setting文件夹内的org.eclipse.wst.common.project.facet.core.xml文件，文件格式大致如下：123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;faceted-project&gt;&lt;runtime name=&quot;Apache Tomcat v5.5&quot;/&gt;&lt;fixed facet=&quot;jst.web&quot;/&gt;&lt;fixed facet=&quot;jst.java&quot;/&gt;&lt;installed facet=&quot;jst.java&quot; version=&quot;5.0&quot;/&gt;&lt;installed facet=&quot;jst.web&quot; version=&quot;3.0&quot;/&gt;&lt;installed facet=&quot;wst.jsdt.web&quot; version=&quot;1.0&quot;/&gt;&lt;/faceted-project&gt; 直接手动修改jst.web对应的version即可。","tags":[{"name":"java","slug":"java","permalink":"www.hzzzy.top/tags/java/"}]},{"title":"在linux下sqlldr的安装","date":"2017-06-25T03:43:18.000Z","path":"2017/06/25/sqlldr-install/","text":"有时需要在某台服务器（以下简称为客服端）上安装sqlldr，方便导数据。 Oracle sqlldr 的配置步骤如下： 1.从官方下载以下包： instantclient-basic-linux.x64-11.2.0.4.0.zip instantclient-jdbc-linux.x64-11.2.0.4.0.zip instantclient-sdk-linux.x64-11.2.0.4.0.zip instantclient-sqlplus-linux.x64-11.2.0.4.0.zip 2.在客服端的服务器上创建/opt/oracle目录，并将以上四个压缩包解压12345sudo mkdir /opt/oraclesudo unzip instantclient-basic-linux.x64-11.2.0.4.0.zip -d /opt/oraclesudo unzip instantclient-jdbc-linux.x64-11.2.0.4.0.zip -d /opt/oraclesudo unzip instantclient-sdk-linux.x64-11.2.0.4.0.zip -d /opt/oraclesudo unzip instantclient-sqlplus-linux.x64-11.2.0.4.0.zip -d /opt/oracle 3.创建/opt/oracle/instantclient_11_2/tnsnames.ora，并配置以下内容：12345678CLEARING =(DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = *.*.*.*)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = ***) ) ) 4.在客服端的服务器上配置环境变量：1sudo vi /etc/profile.d/oracle.sh 添加以下内容：1234567export NLS_LANG=AMERICAN_AMERICA.ZHS16GBKexport ORACLE_IC_HOME=/opt/oracle/instantclient_11_2export ORACLE_HOME=$ORACLE_IC_HOMEexport TNS_ADMIN=$ORACLE_IC_HOMEexport PATH=$PATH:$HOME/bin:$ORACLE_IC_HOMEexport LD_LIBRARY_PATH=$ORACLE_IC_HOME:/usr/libexport ORACLE_SID=CLEARING 然后执行：1source /etc/profile 5.在客服端的服务器上创建以下目录：1mkdir -p /opt/oracle/instantclient_11_2/rdbms/mesg 6.从已安装的oracle服务端上的目录/app/oracle/product/11.2.0/db_1/rdbms/mesg下获取文件ulus.msb，并放置在客服端的服务器上的目录/opt/oracle/instantclient_11_2/rdbms/mesg下。7.从已安装的oracle服务端上的/app/oracle/product/11.2.0/db_1/bin/目录下获取文件sqlldr，并放置在客服端的服务器上的目录/opt/oracle/instantclient_11_2下，同时给予sqlldr程序执行权限，1chmod +X /opt/oracle/instantclient_11_2/sqlldr","tags":[{"name":"sqlldr","slug":"sqlldr","permalink":"www.hzzzy.top/tags/sqlldr/"}]},{"title":"jQuery实际生产中常用的方法","date":"2017-06-24T12:14:30.000Z","path":"2017/06/24/jquery-common/","text":"常常忘了，这语法怎么写的，那个是怎么写的。只是模糊有个影子，却又写不出个完整，最后得查一推文档，才找到自己想要的。现把常用的方法收集整理成一个文档，默记。 1、解决自定义方法或其他类库与jQuery的冲突很多时候我们自己定义了$(id)方法来获取一个元素，或者其他的一些js类库如prototype也都定义了$方法，如果同时把这些内容放在一起就会引起变量方法定义冲突，Jquery对此专门提供了方法用于解决此问题。使用jquery中的jQuery.noConflict();方法即可把变量$的控制权让渡给第一个实现它的那个库或之前自定义的$方法。之后应用Jquery的时候只要将所有的$换成jQuery即可，如原来引用对象方法$(“#msg”)改为jQuery(“#msg”)。如：12345jQuery.noConflict(); // 开始使用jQuery jQuery(&quot;div p&quot;).hide(); // 使用其他库的 $() $(&quot;content&quot;).style.display = &apos;none&apos;; 2、页面元素的引用通过jquery的$()引用元素包括通过id、class、元素名以及元素的层级关系及dom或者xpath条件等方法，且返回的对象为jquery对象（集合对象），不能直接调用dom定义的方法。 3、获取select下拉框的值1$(&quot;#select&quot;).val() 4、获取单选radio的选中值，三种方法都可以：123$(&apos;input:radio:checked&apos;).val()；$(&quot;input[type=&apos;radio&apos;]:checked&quot;).val();$(&quot;input[name=&apos;rd&apos;]:checked&quot;).val(); 5、获取多选checkbox选中的值123456var test =[];$(&quot;input[name=test]&quot;).each(function() &#123; if ($(this).attr(&quot;checked&quot;)) &#123; test.push($(this).val()); &#125; &#125;); 6、操作元素的样式1234567$(&quot;#test&quot;).css(&quot;background&quot;); //返回元素的背景颜色$(&quot;#test&quot;).css(&quot;background&quot;,&quot;#ccc&quot;) //设定元素背景为灰色$(&quot;#test&quot;).height(300); $(&quot;#msg&quot;).width(&quot;200&quot;); //设定宽高$(&quot;#test&quot;).css(&#123; color: &quot;red&quot;, background: &quot;blue&quot; &#125;);//以名值对的形式设定样式$(&quot;#test&quot;).addClass(&quot;select&quot;); //为元素增加名称为select的class$(&quot;#test&quot;).removeClass(&quot;select&quot;); //删除元素名称为select的class$(&quot;#test&quot;).toggleClass(&quot;select&quot;); //如果存在（不存在）就删除（添加）名称为select的class","tags":[{"name":"jQuery","slug":"jQuery","permalink":"www.hzzzy.top/tags/jQuery/"}]},{"title":"部署kafka-zookeeper-storm工程环境","date":"2017-05-21T02:46:55.000Z","path":"2017/05/21/kafka-storm/","text":"实际生产中要处理大量数据，并且是要实时处理的，选择了storm，而数据源则选择与之搭配的kafka。 基础知识 Storm是一个分布式的，可靠的，容错的数据流处理系统。它会把工作任务委托给不同类型的组件，每个组件负责处理一项简单特定的任务。Storm集群的输入流由一个被称作spout的组件管理，spout把数据传递给bolt， bolt要么把数据保存到某种存储器，要么把数据传递给其它的bolt。你可以想象一下，一个Storm集群就是在一连串的bolt之间转换spout传过来的数据。 Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消费。 ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 了解了3个关键的“大零件”，明白他们各自的功能和他们之间的关系，组装一个“粗犷”的大数据环境，也并不是什么难事。以下就是部署是详细步骤了： 1、前置条件1.1 准备多台服务机器（还是选虚拟机吧，个人的话）我这里准备了9台机器，6台给storm用，3台给kafka用，zookeeper也安装在这三台机器上。 机器列表（Ip是假的）： 10.250.11.1 test-kafka1 10.250.11.2 test-kafka2 10.250.11.3 test-kafka3 10.250.11.4 test-storm1 10.250.11.5 test-storm2 10.250.11.6 test-storm3 10.250.11.7 test-storm4 10.250.11.8 test-storm5 10.250.11.9 test-storm6 1.2 关闭防火墙本配置文档中使用的服务器操作系统版本为CentOS7，可在各个服务器上执行以下命令关闭防火墙：1systemctl stop firewalld 2、环境安装2.1 Zookeeper安装配置3个节点的集群zookeeper，节点分布在10.250.11.1，10.250.11.2，10.250.11.3服务器上。步骤如下： 在三台服务器上解压： 1tar -zvxf /home/test/zookeeper-3.4.8.tar.gz -C /home/test &amp;&amp; mv /home/test/zookeeper-3.4.8 /home/test/zookeeper 分别在三台服务器上/home/test/zookeeper下创建data目录，并在conf 目录下添加zoo.cfg配置文件，并配置参数: 1234567891011mkdir /home/test/zookeeper/datacat &gt;&gt; /home/test/zookeeper/conf/zoo.cfg &lt;&lt; EOF tickTime=2000 initLimit=10 syncLimit=5 dataDir=/home/test/zookeeper/data clientPort=2181 server.1=10.250.11.1:2888:3888 server.2=10.250.11.2:2888:3888 server.3=10.250.11.3:2888:3888EOF 在三台服务器的/home/test/zookeeper/data目录下新增myid文件，在文件写入一个数字，代表第几号server，该数字必须核zoo.cfg中配置的server.X一致。 启动集群：分别在三台服务器上启动zookeeper：1/home/test/zookeeper/bin/zkServer.sh start 2.2 kafka安装配置三个节点的kafka集群，节点分布在服务器10.250.11.1，10.250.11.2，10.250.11.3上。在三个服务器上的操作步骤如下： 解压： 1tar -zvxf /home/test/kafka_2.9.2-0.8.1.1.tgz -C /home/test &amp;&amp; mv /home/test/kafka_2.9.2-0.8.1.1 /home/test/kafka 修改配置文件/home/test/kafka/config/server.properties: 1234vi /home/test/kafka/config/server.propertiesbroker.id: 正整数(保证3个节点不同即可.)zookeeper.connect: 10.250.11.3:2181, 10.250.11.2:2181, 10.250.11.1:2181/kafkalog.dirs: /home/test/kafkalogs 创建日志文件存放的目录： 1mkdir /home/test/kafkalogs 启动集群：分别登录三台服务器，并执行以下命令： 1/home/test/kafka/bin/kafka-server-start.sh /home/test/kafka/config/server.properties 登录其中一台kafka节点服务器，创建有效数据topic和异常数据topic：12/home/test/kafka/bin/kafka-topics.sh --create --topic valid_data_topic --replication-factor 3 --partitions 10 --zookeeper 10.250.11.3:2181/kafka/home/test/kafka/bin/kafka-topics.sh --create --topic exeception_topic --replication-factor 3 --partitions 10 --zookeeper 10.250.11.3:2181/kafka 2.3 storm安装配置六个节点的storm集群，分别在服务器10.250.11.4，10.250.11.5，10.250.11.6，10.250.11.7，10.250.11.8，10.250.11.9上。在服务器上的操作步骤如下： 解压： 1tar -zvxf /home/test/apache-storm-1.0.1.tar.gz -C /home/test &amp;&amp; mv /home/test/apache-storm-1.0.1 /home/test/storm 创建配置文件/home/test/storm/conf/storm.yaml，并增加配置信息： 12345678910111213storm.zookeeper.servers: - &quot;10.250.11.1&quot; - &quot;10.250.11.2&quot; - &quot;10.250.11.3&quot;storm.zookeeper.port: 2181worker.childopts: &quot;-Xmx4096m&quot;storm.local.dir: &quot;/home/test/storm/data&quot;nimbus.host: &quot;10.250.11.4&quot;supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 创建storm.local.dir: 1mkdir /home/test/storm/data 启动集群：在10.250.11.4上执行以下命令： 1234/home/test/storm/bin/storm nimbus/home/test/storm/bin/storm supervisor/home/test/storm/bin/storm logviewer/home/test/storm/bin/storm ui 在其余五台服务器上执行以下命令：12/home/test/storm/bin/storm supervisor/home/test/storm/bin/storm logviewer 至此为止，kafka-zookeeper-storm的环境已部署完整，接下来，你就可以敲代码了，嘀嘀嘀。。。","tags":[{"name":"storm","slug":"storm","permalink":"www.hzzzy.top/tags/storm/"}]},{"title":"MySQL创建用户与授权","date":"2017-05-18T02:20:47.000Z","path":"2017/05/18/mysql-order-user/","text":"一. 创建用户命令:1CREATE USER &apos;username&apos;@&apos;host&apos; IDENTIFIED BY &apos;password&apos;; 说明： username：你将创建的用户名 host：指定该用户在哪个主机上可以登陆，如果是本地用户可用localhost，如果想让该用户可以从任意远程主机登陆，可以使用通配符% password：该用户的登陆密码，密码可以为空，如果为空则该用户可以不需要密码登陆服务器 例子：12345CREATE USER &apos;dog&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;;CREATE USER &apos;pig&apos;@&apos;192.168.1.101_&apos; IDENDIFIED BY &apos;123456&apos;;CREATE USER &apos;pig&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;CREATE USER &apos;pig&apos;@&apos;%&apos; IDENTIFIED BY &apos;&apos;;CREATE USER &apos;pig&apos;@&apos;%&apos;; 二. 授权:命令:1GRANT privileges ON databasename.tablename TO &apos;username&apos;@&apos;host&apos; 说明: privileges：用户的操作权限，如SELECT，INSERT，UPDATE等，如果要授予所的权限则使用ALL databasename：数据库名 tablename：表名，如果要授予该用户对所有数据库和表的相应操作权限则可用表示，如.*例子:123GRANT SELECT, INSERT ON test.user TO &apos;pig&apos;@&apos;%&apos;;GRANT ALL ON *.* TO &apos;pig&apos;@&apos;%&apos;;GRANT ALL ON maindataplus.* TO &apos;pig&apos;@&apos;%&apos;; 注意:用以上命令授权的用户不能给其它用户授权，如果想让该用户可以授权，用以下命令:1GRANT privileges ON databasename.tablename TO &apos;username&apos;@&apos;host&apos; WITH GRANT OPTION; 三.设置与更改用户密码命令:1SET PASSWORD FOR &apos;username&apos;@&apos;host&apos; = PASSWORD(&apos;newpassword&apos;); 如果是当前登陆用户用:1SET PASSWORD = PASSWORD(&quot;newpassword&quot;); 例子:1SET PASSWORD FOR &apos;pig&apos;@&apos;%&apos; = PASSWORD(&quot;123456&quot;); 四. 撤销用户权限命令:1REVOKE privilege ON databasename.tablename FROM &apos;username&apos;@&apos;host&apos;; 说明:privilege, databasename, tablename：同授权部分例子:1REVOKE SELECT ON *.* FROM &apos;pig&apos;@&apos;%&apos;; 注意:假如你在给用户‘pig‘@’%’授权的时候是这样的（或类似的）：GRANT SELECT ON test.user TO ‘pig‘@’%’，则在使用REVOKE SELECT ON . FROM ‘pig‘@’%’;命令并不能撤销该用户对test数据库中user表的SELECT 操作。相反，如果授权使用的是GRANT SELECT ON . TO ‘pig‘@’%’;则REVOKE SELECT ON test.user FROM ‘pig‘@’%’;命令也不能撤销该用户对test数据库中user表的Select权限。 具体信息可以用命令SHOW GRANTS FOR ‘pig‘@’%’; 查看。五.删除用户命令:1DROP USER &apos;username&apos;@&apos;host&apos;;","tags":[{"name":"mysql","slug":"mysql","permalink":"www.hzzzy.top/tags/mysql/"}]},{"title":"flask 初学入门","date":"2017-05-12T02:35:40.000Z","path":"2017/05/12/flask/","text":"Flask是一个Python编写的Web 微框架，让我们可以使用Python语言快速实现一个网站或Web服务。本文参考自Flask官方文档，大部分代码引用自官方文档。 安装Flask最简单方法使用pip安装pip install flask然后打开一个Python文件，输入下面的内容并运行该文件。然后访问localhost:5000(Flask默认端口是5000)，我们应当可以看到浏览器上输出了Hello Flask!。 1234567891011from flask import Flaskapp = Flask(__name__)@app.route(&apos;/&apos;)def hello_world(): return &apos;Hello Flask!&apos;if __name__ == &apos;__main__&apos;: app.run() 学习知识点路由在上面的例子里可以看到路由的使用。如果了解Spring Web MVC的话，应该对路由很熟悉。路由通过使用Flask的app.route装饰器来设置，这类似Java的注解。1234567@app.route(&apos;/&apos;)def index(): return &apos;Index Page&apos;@app.route(&apos;/hello&apos;)def hello(): return &apos;Hello, World&apos; 路径变量如果希望获取/article/1这样的路径参数，就需要使用路径变量。路径变量的语法是/path/converter:varname。在路径变量前还可以使用可选的转换器，有以下几种转换器。 转换器 作用 string 默认选项，接受除了斜杠之外的字符串 int 接受整数 float 接受浮点数 path 和string类似，不过可以接受带斜杠的字符串 any 匹配任何一种转换器 uuid 接受UUID字符串 下面是Flask官方的例子。123456789@app.route(&apos;/user/&lt;username&gt;&apos;)def show_user_profile(username): # show the user profile for that user return &apos;User %s&apos; % username@app.route(&apos;/post/&lt;int:post_id&gt;&apos;)def show_post(post_id): # show the post with the given id, the id is an integer return &apos;Post %d&apos; % post_id 构造URL在Web程序中常常需要获取某个页面的URL，在Flask中需要使用url_for(‘方法名’)来构造对应方法的URL。下面是Flask官方的例子。123456789101112131415161718192021&gt;&gt;&gt; from flask import Flask, url_for&gt;&gt;&gt; app = Flask(__name__)&gt;&gt;&gt; @app.route(&apos;/&apos;)... def index(): pass...&gt;&gt;&gt; @app.route(&apos;/login&apos;)... def login(): pass...&gt;&gt;&gt; @app.route(&apos;/user/&lt;username&gt;&apos;)... def profile(username): pass...&gt;&gt;&gt; with app.test_request_context():... print url_for(&apos;index&apos;)... print url_for(&apos;login&apos;)... print url_for(&apos;login&apos;, next=&apos;/&apos;)... print url_for(&apos;profile&apos;, username=&apos;John Doe&apos;)...//login/login?next=//user/John%20Doe HTTP方法如果需要处理具体的HTTP方法，在Flask中也很容易，使用route装饰器的methods参数设置即可。12345678from flask import request@app.route(&apos;/login&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;])def login(): if request.method == &apos;POST&apos;: do_the_login() else: show_the_login_form() 静态文件Web程序中常常需要处理静态文件，在Flask中需要使用url_for函数并指定static端点名和文件名。在下面的例子中，实际的文件应放在static/文件夹下。url_for(&#39;static&#39;, filename=&#39;style.css&#39;) 模板生成Flask默认使用Jinja2作为模板，Flask会自动配置Jinja 模板，所以我们不需要其他配置了。默认情况下，模板文件需要放在templates文件夹下。 使用 Jinja 模板，只需要使用render_template函数并传入模板文件名和参数名即可。123456from flask import render_template@app.route(&apos;/hello/&apos;)@app.route(&apos;/hello/&lt;name&gt;&apos;)def hello(name=None): return render_template(&apos;hello.html&apos;, name=name) 相应的模板文件hello.html如下:1234567&lt;!doctype html&gt;&lt;title&gt;Hello from Flask&lt;/title&gt;&#123;% if name %&#125; &lt;h1&gt;Hello &#123;&#123; name &#125;&#125;!&lt;/h1&gt;&#123;% else %&#125; &lt;h1&gt;Hello, World!&lt;/h1&gt;&#123;% endif %&#125; 日志输出Flask 为我们预配置了一个 Logger，我们可以直接在程序中使用。这个Logger是一个标准的Python Logger，所以我们可以向标准Logger那样配置它，详情可以参考官方文档或者我的文章Python 日志输出。 123app.logger.debug(&apos;A value for debugging&apos;)app.logger.warning(&apos;A warning occurred (%d apples)&apos;, 42)app.logger.error(&apos;An error occurred&apos;) 处理请求在 Flask 中获取请求参数需要使用request等几个全局对象，但是这几个全局对象比较特殊，它们是 Context Locals ，其实就是 Web 上下文中局部变量的代理。虽然我们在程序中使用的是全局变量，但是对于每个请求作用域，它们都是互不相同的变量。理解了这一点，后面就非常简单了。 Request 对象Request 对象是一个全局对象，利用它的属性和方法，我们可以方便的获取从页面传递过来的参数。 method属性会返回HTTP方法的类似，例如post和get。form属性是一个字典，如果数据是POST类型的表单，就可以从form属性中获取。下面是 Flask 官方的例子，演示了 Request 对象的method和form属性。1234567891011121314from flask import request@app.route(&apos;/login&apos;, methods=[&apos;POST&apos;, &apos;GET&apos;])def login(): error = None if request.method == &apos;POST&apos;: if valid_login(request.form[&apos;username&apos;], request.form[&apos;password&apos;]): return log_the_user_in(request.form[&apos;username&apos;]) else: error = &apos;Invalid username/password&apos; # the code below is executed if the request method # was GET or the credentials were invalid return render_template(&apos;login.html&apos;, error=error) 如果数据是由GET方法传送过来的，可以使用args属性获取，这个属性也是一个字典。searchword = request.args.get(&#39;key&#39;) 文件上传利用Flask也可以方便的获取表单中上传的文件，只需要利用 request 的files属性即可，这也是一个字典，包含了被上传的文件。如果想获取上传的文件名，可以使用filename属性，不过需要注意这个属性可以被客户端更改，所以并不可靠。更好的办法是利用werkzeug提供的secure_filename方法来获取安全的文件名。12345678from flask import requestfrom werkzeug.utils import secure_filename@app.route(&apos;/upload&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;])def upload_file(): if request.method == &apos;POST&apos;: f = request.files[&apos;the_file&apos;] f.save(&apos;/var/www/uploads/&apos; + secure_filename(f.filename)) 响应处理默认情况下，Flask会根据函数的返回值自动决定如何处理响应：如果返回值是响应对象，则直接传递给客户端；如果返回值是字符串，那么就会将字符串转换为合适的响应对象。我们也可以自己决定如何设置响应对象，方法也很简单，使用make_response函数即可。12345@app.errorhandler(404)def not_found(error): resp = make_response(render_template(&apos;error.html&apos;), 404) resp.headers[&apos;X-Something&apos;] = &apos;A value&apos; return resp CookiesFlask也可以方便的处理Cookie。使用方法很简单，直接看官方的例子就行了。下面的例子是如何获取cookie。1234567from flask import request@app.route(&apos;/&apos;)def index(): username = request.cookies.get(&apos;username&apos;) # 使用 cookies.get(key) 代替 cookies[key] 避免 # 得到 KeyError 如果cookie不存在 如果需要发送cookie给客户端，参考下面的例子。1234567from flask import make_response@app.route(&apos;/&apos;)def index(): resp = make_response(render_template(...)) resp.set_cookie(&apos;username&apos;, &apos;the username&apos;) return resp Sessions我们可以使用全局对象session来管理用户会话。Sesison 是建立在 Cookie 技术上的，不过在 Flask 中，我们还可以为 Session 指定密钥，这样存储在 Cookie 中的信息就会被加密，从而更加安全。直接看 Flask 官方的例子吧。123456789101112131415161718192021222324252627282930from flask import Flask, session, redirect, url_for, escape, requestapp = Flask(__name__)@app.route(&apos;/&apos;)def index(): if &apos;username&apos; in session: return &apos;Logged in as %s&apos; % escape(session[&apos;username&apos;]) return &apos;You are not logged in&apos;@app.route(&apos;/login&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;])def login(): if request.method == &apos;POST&apos;: session[&apos;username&apos;] = request.form[&apos;username&apos;] return redirect(url_for(&apos;index&apos;)) return &apos;&apos;&apos; &lt;form method=&quot;post&quot;&gt; &lt;p&gt;&lt;input type=text name=username&gt; &lt;p&gt;&lt;input type=submit value=Login&gt; &lt;/form&gt; &apos;&apos;&apos;@app.route(&apos;/logout&apos;)def logout(): # remove the username from the session if it&apos;s there session.pop(&apos;username&apos;, None) return redirect(url_for(&apos;index&apos;))# set the secret key. keep this really secret:app.secret_key = &apos;A0Zr98j/3yX R~XHH!jmN]LWX/,?RT&apos; 重定向和错误redirect和abort函数用于重定向和返回错误页面。12345678910from flask import abort, redirect, url_for@app.route(&apos;/&apos;)def index(): return redirect(url_for(&apos;login&apos;))@app.route(&apos;/login&apos;)def login(): abort(401) this_is_never_executed() 默认的错误页面是一个空页面，如果需要自定义错误页面，可以使用errorhandler装饰器。12345from flask import render_template@app.errorhandler(404)def page_not_found(error): return render_template(&apos;page_not_found.html&apos;), 404 模板简介Jinja 模板的使用方法，详细资料请直接看原文档","tags":[{"name":"Flask","slug":"Flask","permalink":"www.hzzzy.top/tags/Flask/"}]},{"title":"python基础","date":"2017-04-23T01:42:34.000Z","path":"2017/04/23/python-base/","text":"当初学python，是为了搞爬虫，到互联网上爬取自己想要的资料。个人认为，学好了一种编程语言，再转学另外一种，是很快上手的，毕竟，原理懂了，万变不离其中。python的基础教程网上一大把，但如果要深入精通的，还是要跟随一定的python项目，方得修炼成形。基础教程推荐 廖雪峰的 《Python教程》 学了基础后，其实，当时是很懂的，然后过了几天，什么语法呀，函数呀，模块呀，忘记了。模模糊糊中，只有似曾相似的感觉，大抵这就是不常用导致的，本人不是主python，只在需要的时候，拿起python就是干！方便不去翻python文档，只需瞄一眼，就知道怎么回事的，唯有下面这张图了（来源于网络）：","tags":[{"name":"python","slug":"python","permalink":"www.hzzzy.top/tags/python/"}]},{"title":"Linux下安装MySQL","date":"2017-04-18T02:00:15.000Z","path":"2017/04/18/install-mysql/","text":"安装步骤一：准备安装包 下载地址：http://dev.mysql.com/downloads/mysql/5.6.html#downloads 下载版本：我这里选择的5.6.33，通用版，linux下64位 也可以直接复制64位的下载地址，通过命令下载：wget http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz 二：着手安装(在root用户下操作)1.在安装包存放目录下执行命令解压文件1tar -zxvf mysql-5.6.31-linux-glibc2.5-x86_64.tar.gz 2.删除安装包，重命名解压后的文件12rm -f mysql-5.6.31-linux-glibc2.5-x86_64.tar.gzmv mysql-5.6.31-linux-glibc2.5-x86_64/ /usr/local/mysql 3.添加mysql用户组和mysql用户12groupadd mysqluseradd -r -g mysql mysql 4.进入mysql目录更改权限12cd mysql/chown -R mysql:mysql ./ 5.执行安装脚本1./scripts/mysql_install_db --user=mysql 安装完之后修改当前目录拥有者为root用户，修改data目录拥有者为mysql12chown -R root:root ./chown -R mysql:mysql data 6.启动mysql，更改mysql密码1./support-files/mysql.server start MySQL启动之后再执行如下命令更改密码：1./bin/mysqladmin -u root -h localhost.localdomain password &apos;root&apos; 密码更改后即可登录MySQL1./bin/mysql -h127.0.0.1 -uroot -proot 也可以在登陆后使用命令修改：12update mysql.user set password=password(&apos;root&apos;) where user=&apos;root&apos;;flush privileges; 7.增加远程登录权限12grant all privileges on *.* to root@&apos;%&apos; identified by &apos;root&apos;;flush privileges; 8.将MySQL加入Service系统服务123cp support-files/mysql.server /etc/init.d/mysqldchkconfig --add mysqldchkconfig mysqld on Service 命令：1234service mysqld startservice mysqld restartservice mysqld statusservice mysqld stop 9.配置my.cnf12345vim my.cnf#添加以下两条语句并保存退出character_set_server=utf8lower_case_table_names=1max_allowed_packet=100M 配置好之后，要重启mysqld服务10.外门：杀MySQL进程1234ps aux|grep mysqlkill -9 上边的进程号#或者下边一条命令即可杀掉所有MySQL进程ps aux|grep mysql|awk &apos;&#123;print $2&#125;&apos;|xargs kill -9","tags":[{"name":"mysql","slug":"mysql","permalink":"www.hzzzy.top/tags/mysql/"}]},{"title":"Synchronized的用法","date":"2017-04-02T03:51:07.000Z","path":"2017/04/02/Synchronized/","text":"Synchronized的概念synchronized是Java中的关键字，是一种同步锁。它修饰的对象有以下几种：1.修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象;2.修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象;3.修改一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象；4.修改一个类，其作用的范围是synchronized后面括号括起来的部分，作用的对象是这个类的所有对象; 修饰一个代码块 一个线程访问一个对象中的synchronized(this)同步代码块时，其他试图访问该对象的线程将被阻塞。我们看下面一个例子： 调用SyncThread：12345SyncThread syncThread = new SyncThread();Thread thread1 = new Thread(syncThread, &quot;SyncThread1&quot;);Thread thread2 = new Thread(syncThread, &quot;SyncThread2&quot;);thread1.start();thread2.start(); 结果如下:12345678910SyncThread1:0SyncThread1:1SyncThread1:2SyncThread1:3SyncThread1:4SyncThread2:5SyncThread2:6SyncThread2:7SyncThread2:8SyncThread2:9 当两个并发线程(thread1和thread2)访问同一个对象(syncThread)中的synchronized代码块时，在同一时刻只能有一个线程得到执行，另一个线程受阻塞，必须等待当前线程执行完这个代码块以后才能执行该代码块。Thread1和thread2是互斥的，因为在执行synchronized代码块时会锁定当前的对象，只有执行完该代码块才能释放该对象锁，下一个线程才能执行并锁定该对象。 但如果是两个不同对象呢?并发线程调用不同的对象,会发生什么呢?稍微修改一下调用SyncThread的方法:123456SyncThread syncThread1 = new SyncThread();SyncThread syncThread2 = new SyncThread();Thread thread1 = new Thread(syncThread1, &quot;SyncThread1&quot;);Thread thread2 = new Thread(syncThread2, &quot;SyncThread2&quot;);thread1.start();thread2.start(); 结果如下:12345678910SyncThread1:0SyncThread2:1SyncThread1:2SyncThread2:3SyncThread1:4SyncThread2:5SyncThread1:6SyncThread2:7SyncThread1:8SyncThread2:9 不是说一个线程执行synchronized代码块时其它的线程受阻塞吗？为什么上面的例子中thread1和thread2同时在执行。这是因为synchronized只锁定对象，每个对象只有一个锁（lock）与之相关联，而上面的代码等同于下面这段代码：123456SyncThread syncThread1 = new SyncThread();SyncThread syncThread2 = new SyncThread();Thread thread1 = new Thread(syncThread1, &quot;SyncThread1&quot;);Thread thread2 = new Thread(syncThread2, &quot;SyncThread2&quot;);thread1.start();thread2.start(); 这时创建了两个SyncThread的对象syncThread1和syncThread2，线程thread1执行的是syncThread1对象中的synchronized代码(run)，而线程thread2执行的是syncThread2对象中的synchronized代码(run)；我们知道synchronized锁定的是对象，这时会有两把锁分别锁定syncThread1对象和syncThread2对象，而这两把锁是互不干扰的，不形成互斥，所以两个线程可以同时执行。 2.当一个线程访问对象的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该对象中的非synchronized(this)同步代码块。[demo2]:1234567891011121314151617181920212223242526272829303132333435363738394041class Counter implements Runnable&#123; private int count; public Counter() &#123; count = 0; &#125; public void countAdd() &#123; synchronized(this) &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //非synchronized代码块，未对count进行读写操作，所以可以不用synchronized public void printCount() &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot; count:&quot; + count); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Override public void run() &#123; String threadName = Thread.currentThread().getName(); if (threadName.equals(&quot;A&quot;)) &#123; countAdd(); &#125; else if (threadName.equals(&quot;B&quot;)) &#123; printCount(); &#125; &#125;&#125; 调用代码如下:12345Counter counter = new Counter();Thread thread1 = new Thread(counter, &quot;A&quot;);Thread thread2 = new Thread(counter, &quot;B&quot;);thread1.start();thread2.start(); 执行结果如下:12345678910A:0 B count:1 A:1 B count:2 A:2 B count:3 A:3 B count:4 A:4 B count:5 上面代码中countAdd是一个synchronized的，printCount是非synchronized的。从上面的结果中可以看出一个线程访问一个对象的synchronized代码块时，别的线程可以访问该对象的非synchronized代码块而不受阻塞。 修饰一个方法Synchronized修饰一个方法很简单，就是在方法的前面加synchronized，public synchronized void method(){//todo}; synchronized修饰方法和修饰一个代码块类似，只是作用范围不一样，修饰代码块是大括号括起来的范围，而修饰方法范围是整个函数。如将【Demo1】中的run方法改成如下的方式，实现的效果一样。[demo3]:12345678910public synchronized void run() &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Synchronized作用于整个方法的写法。写法一：1234public synchronized void method()&#123; // todo&#125; 写法二：123456public void method()&#123; synchronized(this) &#123; // todo &#125;&#125; 写法一修饰的是一个方法，写法二修饰的是一个代码块，但写法一与写法二是等价的，都是锁定了整个方法时的内容。 在用synchronized修饰方法时要注意以下几点： synchronized关键字不能继承。虽然可以使用synchronized来定义方法，但synchronized并不属于方法定义的一部分，因此，synchronized关键字不能被继承。如果在父类中的某个方法使用了synchronized关键字，而在子类中覆盖了这个方法，在子类中的这个方法默认情况下并不是同步的，而必须显式地在子类的这个方法中加上synchronized关键字才可以。当然，还可以在子类方法中调用父类中相应的方法，这样虽然子类中的方法不是同步的，但子类调用了父类的同步方法，因此，子类的方法也就相当于同步了。这两种方式的例子代码如下：-在子类方法中加上synchronized关键字:123456class Parent &#123; public synchronized void method() &#123; &#125;&#125;class Child extends Parent &#123; public synchronized void method() &#123; &#125;&#125; -在子类方法中调用父类的同步方法123456class Parent &#123; public synchronized void method() &#123; &#125;&#125;class Child extends Parent &#123; public void method() &#123; super.method(); &#125;&#125; 2.在定义接口方法时不能使用synchronized关键字。3.构造方法不能使用synchronized关键字，但可以使用synchronized代码块来进行同步。 修饰一个静态的方法Synchronized也可修饰一个静态方法，用法如下：123public synchronized static void method() &#123; // todo&#125; 我们知道静态方法是属于类的而不属于对象的。同样的，synchronized修饰的静态方法锁定的是这个类的所有对象。我们对Demo1进行一些修改如下：[demo4]:12345678910111213141516171819202122class SyncThread implements Runnable &#123; private static int count; public SyncThread() &#123; count = 0; &#125; public synchronized static void method() &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Override public void run() &#123; method(); &#125;&#125; 调用代码如下:123456SyncThread syncThread1 = new SyncThread();SyncThread syncThread2 = new SyncThread();Thread thread1 = new Thread(syncThread1, &quot;SyncThread1&quot;);Thread thread2 = new Thread(syncThread2, &quot;SyncThread2&quot;);thread1.start();thread2.start(); 执行的结果如下:12345678910SyncThread1:0 SyncThread1:1 SyncThread1:2 SyncThread1:3 SyncThread1:4 SyncThread2:5 SyncThread2:6 SyncThread2:7 SyncThread2:8 SyncThread2:9 syncThread1和syncThread2是SyncThread的两个对象，但在thread1和thread2并发执行时却保持了线程同步。这是因为run中调用了静态方法method，而静态方法是属于类的，所以syncThread1和syncThread2相当于用了同一把锁。这与Demo1是不同的。 修饰一个类Synchronized还可作用于一个类，用法如下：1234567class ClassName &#123; public void method() &#123; synchronized(ClassName.class) &#123; // todo &#125; &#125;&#125; 其效果和【Demo4】是一样的，synchronized作用于一个类T时，是给这个类T加锁，T的所有对象用的是同一把锁。 总结: 无论synchronized关键字加在方法上还是对象上，如果它作用的对象是非静态的，则它取得的锁是对象；如果synchronized作用的对象是一个静态方法或一个类，则它取得的锁是对类，该类所有的对象同一把锁。 每个对象只有一个锁（lock）与之相关联，谁拿到这个锁谁就可以运行它所控制的那段代码。 实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制。","tags":[{"name":"java","slug":"java","permalink":"www.hzzzy.top/tags/java/"}]},{"title":"linux下mongodb的安装","date":"2017-03-21T06:22:54.000Z","path":"2017/03/21/mongodb-istall/","text":"导读 MongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。 MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 在 x.x.x.x 服务器上配置mongodb，具体步骤如下： 1.解压：1tar -zxvf /home/test/mongodb-linux-x86_64-rhel70-v3.4-latest.tgz -C /usr/local &amp;&amp; mv /usr/local/mongodb-linux-x86_64-rhel70-v3.4-latest /usr/local/mongodb 2.创建log以及data目录：12mkdir /usr/local/mongodb/datamkdir /usr/local/mongodb/log 3.创建配置文件/usr/local/mongodb/mongodb.conf，并添加如下内容：123456port=27017dbpath=/usr/local/mongodb/datalogpath=/usr/local/mongodb/log/mongodb.logfork=truehttpinterface=truerest=true 4.启动mongodb:1/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/mongodb.conf","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"www.hzzzy.top/tags/MongoDB/"}]},{"title":"Hadoop环境搭建教程（2.7.3）","date":"2017-03-12T02:55:43.000Z","path":"2017/03/12/hadoop-env/","text":"1.说明组网：Namenode(172.20.60.145) ——1台Datanode(172.20.60.146-148) ——3台 IP地址与主机名、系统、物理机信息对应如下：172.20.60.145 namenode RHEL6.5 ThinkPad X201i172.20.60.146 datanode101 RHEL6.5 ThinkPad X230虚拟机172.20.60.147 datanode102 RHEL6.5 ThinkPad X230虚拟机172.20.60.148 datanode103 RHEL6.5 ThinkPad X230虚拟机 所需镜像与软件：VMware-workstation-full-9.0.0-812388.zipultraiso_v9.5.3.2901.rarrhel-server-6.5-i386-boot.isorhel-server-6.5-i386-dvd.isojdk-7u71-linux-i586.tar.gzhadoop-2.7.3.tar.gz 2.安装RHEL 6.52.1 安装系统至物理机 2.1.1制作U盘安装盘 使用工具UltraISO打开rhel-server-6.5-i386-boot.iso，执行启动-&gt;写入硬盘映像-&gt;格式化-&gt;写入方式-&gt;便捷启动（写入新的驱动器引导扇区-&gt;Syslinux）-&gt;写入 将rhel-server-6.5-i386-dvd.iso复制到U盘里 2.1.2安装系统 插入U盘，重启电脑，将电脑设置为从U盘启动 根据系统提示安装即可 注：如果在安装过程中选择“使用所有空间”，则电脑上其他操作系统将不可用，慎用 2.1.3 启用网络连接 RHEL6.5安装完毕后默认不启用网络连接，需要手动配置（root用户权限）： 1）若静态地址，则按下面描述配置：12345678910vi /etc/sysconfig/network-scripts/ifcfg-eth0 #编辑配置文件，添加或修改以下内容BOOTPROTO=static #启用静态IP地址ONBOOT=yes #开启自动启用网络连接IPADDR=172.20.60.145 #设置IP地址NETMASK=255.255.255.0 #设置子网掩码GATEWAY=172.20.29.254 #设置网关DNS1=8.8.8.8 #设置主DNS，若网络不通，参照局域网其他PC机的DNS1，如172.20.0.11DNS2=8.8.4.4 #设置备DNS，若网络不通，参照局域网其他PC机的DNS2，如172.20.0.12:wq #保存退出 2）若动态地址，则按下面描述配置： 123456789101112131415vi /etc/sysconfig/network-scripts/ifcfg-eth0 #编辑配置文件，添加或修改以下内容DEVICE=&quot;eth0&quot;BOOTPROTO=&quot;dhcp&quot; #启用动态IP地址HWADDR=&quot;00:0C:29:35:FF:7A&quot;IPV6INIT=&quot;yes&quot;NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;yes&quot; #开启自动启用网络连接TYPE=&quot;Ethernet&quot;UUID=&quot;9f9301b7-f9c2-4911-99fb-29cc5e1f690e&quot;:wq #保存退出service network restart #重启网络连接ping www.hao123.com #测试网络是否正常ifconfig #查看IP地址 2.2安装系统至虚拟机 虚拟机参数：处理器x1，内存256MB，硬盘10~20GB，网络连接选择桥接模式。 打开VMware 9根据提示安装即可。 注：安装时设置内存为512MB以上，否则报内存不足而中断安装，安装完毕之后可根据实际情况更改内存为256MB。 3.Hadoop环境搭建以搭建X201i为例作为指引，虚拟机系统搭建完第一个后，其他2个可以使用克隆，可快速搭建。3.1 服务器配置（需要root用户权限执行） 3.1.1 创建hadoop用户组 groupadd hadoop 3.1.2 创建hadoop用户 useradd -g hadoop hadoop#指定所属组（后一个hadoop为用户名） 3.1.4 为hadoop用户设定密码 passwd hadoop 3.1.3 为hadoop用户添加权限 vi /etc/sudoers#给hadoop用户赋予root相同的权限 hadoop ALL=(ALL) ALL#添加此信息 注意：创建完hadoop用户后，请都使用hadoop用户登录，再进行后续操作。 3.2修改hostname，确定IP host映射关系 3.2.1 修改hostname sudo vi /etc/sysconfig/network #修改成namenode 3.2.2修改hosts文件，确定IP host映射关系 sudo vi /etc/hosts在原有的基础上增加: 172.20.60.145 namenode 172.20.60.146 datanode101 172.20.60.147 datanode102 172.20.60.148 datanode1033.3 ssh无密码登录 在hadoop里面，namenode要通过ssh与datanode进行通信，因此需要安装SSH，并且还需要配置SSH无密码登录，包括本地无密码登录以及namenode远程访问datanode无密码登录。 3.3.1确认SSH安装 rpm -qa |grep openssh rpm -qa |grep rsync 如果没有安装，自行下载安装：sudo rpm –ivh RPM包文件名称。 3.3.2设置本地无密码登录 进入当前用户目录，本文是 /home/hadoop/，ls -a 观察是否有1个.ssh目录，如果没有，请建立该目录（mkdir .ssh） 1、 进入.ssh目录 2、 ssh-keygen -t rsa 之后一路回车（产生秘钥）（如果没有秘钥产生，在前面加上sudo） 3、 把id_rsa.pub 追加到授权的 key 里面去（cat id_rsa.pub &gt;&gt; authorized_keys） 4、 重启 SSH 服务命令使其生效sudo service sshd restart 5、 此时可以进行ssh localhost，验证是否可以本地无密码登录 注：不能无密码访问多半是权限问题：.ssh的权限为700， authorized_keys的权限为600。执行命令： chmod 700 /home/hadoop/.ssh chmod 600 /home/hadoop/.ssh/authorized_keys 3.3.3 设置远程无密码登录 此时还无法做这个事情，要等到datanode安装好才可以，这里先说明下，届时照做即可。 这里所谓的远程无密码登录，是指namenode能够无密码登录到datanode。因此，只需要将namenode中的秘钥（即.ssh目录中的authorized_keys文件）加入到datanode的秘钥中即可。3.4 安装JDK 3.4.1 选择JAVA安装路径 本文中，JAVA安装在/usr/java中，如果没有该目录，使用mkdir java建立 3.4.2 JAVA解压安装 将文件jdk-7u71-linux-i586.tar.gz移动到/usr/java中； 解压安装： tar zxvf jdk-7u71-linux-i586.tar.gz 安装完毕 3.4.3配置JAVA环境变量 sudo vi /etc/profile 在最后面加上如下内容(注意是用冒号分隔)： HADOOP_HOME=/home/hadoop/hadoop-2.7.3 JAVA_HOME=/usr/java/jdk1.7.0_71 CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar PATH=$JAVA_HOME/bin:$PATH:$HADOOP_HOME/bin export JAVA_HOME export CLASSPATH export PATH 然后运行source /etc/profile 通过java -version验证是否安装成功。（java与”-“之间有空格） 3.5关闭防火墙 不关闭防火墙，会导致各主机之间ping不通 查看是否开启： sudo service iptables status 关闭方法： sudo service iptables stop 永远关闭： sudo ntsysv 把iptables前的*号去掉。 附ntsysv操作说明： 上下键：可以在中间的方框当中，在各个服务之间移动； 空格键：可以用来选择你所需要的服务，[*]表示开起启动； tab键：可以在方框、OK、Cancel之间移动； [F1]键：可以显示该服务的说明。 3.6 安装hadoop 3.6.1 解压缩hadoop 将文件hadoop-2.7.3.tar.gz移动到/home/hadoop中； 解压安装：tar -zxvf hadoop-2.7.3.tar.gz 安装完毕 3.6.2 配置hadoop ~/hadoop-2.7.3/etc/hadoop/hadoop-env.sh ~/hadoop-2.7.3/etc/hadoop/yarn-env.sh ~/hadoop-2.7.3/etc/hadoop/slaves ~/hadoop-2.7.3/etc/hadoop/core-site.xml ~/hadoop-2.7.3/etc/hadoop/hdfs-site.xml ~/hadoop-2.7.3/etc/hadoop/yarn-site.xml ~/hadoop-2.7.3/etc/hadoop/mapred-site.xml 以上个别文件默认不存在的，可以复制相应的template文件获得。 3.6.2.1创建目录 配置之前，需要在/home/hadoop/下创建以下目录： mkdir -p /home/hadoop/dfs/name mkdir -p /home/hadoop/dfs/data mkdir -p /home/hadoop/dfs/tmp 3.6.2.2 配置文件1：hadoop-env.sh 修改JAVA_HOME值（export JAVA_HOME=/usr/java/jdk1.7.0_71） 注：尽管默认为export JAVA_HOME=${JAVA_HOME}，也必须修改为绝对路径。 3.6.2.3 配置文件2：yarn-env.sh 将# export JAVA_HOME=/home/y/libexec/jdk1.6.0/修改为： export JAVA_HOME=/usr/java/jdk1.7.0_71 3.6.2.4 配置文件3：slaves 这个文件里面保存所有slave节点 写入以下内容：（这里建议写入hostname。另外，需要将第一行的localhost删除或者注释掉） datanode101 datanode102 datanode103 3.6.2.5 配置文件4：core-site.xml 该文件是设置namenode信息1234567891011121314151617&lt;configuration&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://namenode:9000&lt;/value&gt;&lt;description&gt;The name of the default file system &lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;io.file.buffer.size&lt;/name&gt;&lt;value&gt;131072&lt;/value&gt;&lt;description&gt;Size of read/write buffer used in Sequence Files&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/dfs/tmp&lt;/value&gt;&lt;description&gt;A base for other temporary directories&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 3.6.2.6 配置文件5：hdfs-site.xml 配置HDFS文件系统所在位置1234567891011121314151617181920212223&lt;configuration&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.blocksize&lt;/name&gt;&lt;value&gt;134217728&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;&lt;value&gt;100&lt;/value&gt;&lt;description&gt;The number of server threads for the namenode&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 3.6.2.7 配置文件6：yarn-site.xml 1234567891011121314151617181920212223242526272829303132333435363738&lt;configuration&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;namenode&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;&lt;value&gt;namenode:8032&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;&lt;value&gt;namenode:8030&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;&lt;value&gt;namenode:8031&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;&lt;value&gt;namenode:8033&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;&lt;value&gt;namenode:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.6.2.8配置文件7：mapred-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142&lt;configuration&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;&lt;value&gt;1536&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;&lt;value&gt;-Xmx1024M&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;&lt;value&gt;3072&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;&lt;value&gt;-Xmx2560M&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt;&lt;value&gt;512&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.task.io.sort.factor&lt;/name&gt;&lt;value&gt;100&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt;&lt;value&gt;50&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;namenode:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt; mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;namenode:19888&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.7 搭建虚拟机 按照以上配置重新搭建一台虚拟机，然后使用克隆，快速搭建其他2台虚拟机。克隆之后，依次启动，依次修改。 下面介绍一些克隆完成后，需要做的事情。（以下均都是使用hadoop用户登录） 3.7.1 修改hostname sudo vi /etc/sysconfig/network 3.7.2 修改网卡地址 由于是克隆过来的，所以MAC地址也需要修改，使用ifconfig命令可以查看克隆机MAC地址 sudo vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改完成后，还看不到网口，还需要做这个操作： sudo rm /etc/udev/rules.d/70-persistent-net.rules 重启虚拟机 sudo shutdown -r now 3.7.3 设置SSH远程无密码登录 见3.3.3的方法 4.Hadoop启动与测试进入安装目录： cd ~/hadoop-2.7.3 4.1格式化namenode（只需首次启动时在主节点运行） 进入目录cd ~/hadoop-2.7.3/bin/ ./hdfs namenode -format4.2 启动hadoop 进入目录cd ~/hadoop-2.7.3/sbin/ 依次运行./start-dfs.sh和./start-yarn.sh 根据实际启动MR JobHistory Server:./ mr-jobhistory-daemon.sh start historyserver（默认没有启动） 每次启动之后可以运行jps观察已经启动的服务 4.3 验证状态 查看HDFS: http://172.20.60.145:50070 查看RM: http://172.20.60.145:8088 查看 MR JobHistory Server:http://172.20.60.145: 198884.4 查看各机情况 hdfs dfsadmin -report4.5 测试123456789101112[hadoop@namenode ~]$ echo &quot;hello world hadoop test&quot; &gt; file01[hadoop@namenode ~]$ hadoop fs -mkdir /input[hadoop@namenode ~]$ hadoop fs -mkdir /output[hadoop@namenode ~]$ hadoop fs -put ./file01 /input[hadoop@namenodehadoop-2.7.3]$ echo &quot;hello world&quot; &gt;&gt; file02[hadoop@namenodehadoop-2.7.3]$ hadoop fs -put ./file02 /input[hadoop@namenodehadoop-2.7.3]$ hadoop fs -ls /Found 2 itemsdrwxr-xr-x - hadoop supergroup 0 2014-12-19 08:25 /inputdrwxr-xr-x - hadoop supergroup 0 2014-12-19 08:17 /output[hadoop@namenodehadoop-2.7.3]$ hadoop fs -cat /input/file02hello world 5.附录5.1 虚拟机NAT连接模式设置 本地连接或者无线连接 1 共享网络，选定虚拟网卡vmnet8，主机分配给vmnet8的IP地址一般为：192.168.x.1，x=1~244，子网掩码为255.255.255.0 2 NAT设置，打开虚拟机–编辑–虚拟网络编辑器，选定vmnet8，点击NAT设置，确定网关为192.168.x.2；点击DHCP设置，确定动态分配地址为192.168.x.128~192.168.x.254，即192.168.x.3~192.168.x.127为静态地址，必须指定static. 3如果选定DHCP，则无需进入系统进行设置，直接上网；如果指定静态地址，则相关信息为如下所示： BOOTPROTO：static IP：192.168.x.3~192.168.x.127 子网掩码：255.255.255.0 网关：192.168.x.2 DNS1：跟主机一样 DNS2：跟主机一样5.2 挂载和卸载U盘命令 挂载U盘 mount -t vfat /dev/sdb1 /media/disk 卸载U盘 umount /media/disk或者umount /dev/sdb15.3 因profile配置文件错误而无法使用LINUX命令解决方法 执行export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin5.4 执行文件 如果path中有你的程序所在的目录，那么直接执行filename即可 如果path中没有程序所在目录，那么进入目录./filename或者path/filename5.5 IP地址更换后续事情 修改hosts文件：sudo vi /etc/hosts 配置ssh无密码登录 重启hadoop（或许不需要重启）5.6 格式化namenode引起的clusterID不一致 删除该数据节点的VERSION即可：rm /home/hadoop/dfs/data/current/VERSION 重启hadoop（或许不需要重启）5.7 虚拟机与物理机共享文件 安装VMware tools：虚拟机-&gt;安装VMware tools-&gt; Linux下安装（注：虚拟机默认安装此工具，若未安装再点击安装） 设置共享文件夹：虚拟机-&gt;设置-&gt;选项-&gt;共享文件夹-&gt;总是启用-&gt;添加，如添加共享文件夹为E:\\RHEL\\share 虚拟机系统对应目录：/mnt/hgfs/share","tags":[{"name":"hadoop","slug":"hadoop","permalink":"www.hzzzy.top/tags/hadoop/"}]},{"title":"解决笔记本ubuntu系统外接显示器分辨率的问题","date":"2017-01-16T09:24:14.000Z","path":"2017/01/16/xrandr-order/","text":"笔记本装的是ubuntu系统，外接一个大显示器，看起来舒服点，无奈，接显示器后，分辨率变低了，屏幕只显示了一半最终使用xrandr命令解决了问题。 步骤如下：1、先用xrandr命令查看1234567891011121314151617181920212223242526272829hzzzy@hzzzy:~$ xrandrScreen 0: minimum 8 x 8, current 1366 x 768, maximum 32767 x 32767LVDS1 connected 1366x768+0+0 (normal left inverted right x axis y axis) 344mm x 194mm 1366x768 60.00*+ 1360x768 59.80 59.96 1280x720 60.00 1024x768 60.00 1024x576 60.00 960x540 60.00 800x600 60.32 56.25 864x486 60.00 640x480 59.94 720x405 60.00 680x384 60.00 640x360 60.00 DP1 disconnected (normal left inverted right x axis y axis)HDMI1 disconnected (normal left inverted right x axis y axis)VGA1 connected 1024x768+0+0 (normal left inverted right x axis y axis) 408mm x 255mm 1440x900 59.89 + 74.98 1280x1024 75.02 60.02 1280x960 60.00 1280x800 74.93 59.81 1152x864 75.00 1024x768 75.08* 70.07 60.00 832x624 74.55 800x600 72.19 75.00 60.32 56.25 640x480 75.00 72.81 66.67 60.00 720x400 70.08VIRTUAL1 disconnected (normal left inverted right x axis y axis) 上面标有*的是当前的显示器分辨率。LVDS1是我笔记本的，VGA1是我外接的显示器的分辨率。 2、从上面可以看到，我的外接的显示器分辨率可达到1440x900的，执行cvt123hzzzy@hzzzy:~$ cvt 1440 900# 1440x900 59.89 Hz (CVT 1.30MA) hsync: 55.93 kHz; pclk: 106.50 MHzModeline &quot;1440x900_60.00&quot; 106.50 1440 1528 1672 1904 900 903 909 934 -hsync +vsync 3、Modeline后面的参数就是要设置的显示器的分辨率的参数,执行sudo xrandr –newmode 命令1sudo xrandr --newmode &quot;1440x900_60.00&quot; 106.50 1440 1528 1672 1904 900 903 909 934 -hsync +vsync 4、把分辨率mode赋予外接的显示器1sudo xrandr --addmode VGA1 &quot;1440x900_60.00&quot; 5、设置输出外接显示器1sudo xrandr --output VGA1 --mode &quot;1440x900_60.00&quot; 通过以上5部可解决ubutnu系统外接显示器分辨率的问题。但，注意了，重启或关机再开机后，以上的设置会失效。为了以后都使用这方式解决分辨率的问题，可把以上的命令做成shell脚本，并随系统启动而启动。","tags":[{"name":"问题","slug":"问题","permalink":"www.hzzzy.top/tags/问题/"}]},{"title":"git使用过程中遇到的问题－持续更新","date":"2017-01-16T01:19:06.000Z","path":"2017/01/16/git-problems/","text":"如果系统中有一些配置文件在服务器上做了配置修改,然后后续开发又新添加一些配置项的时候,在发布这个配置文件的时候,会发生代码冲突:123error: Your local changes to the following files would be overwritten by merge: protected/config/mytest.pyPlease, commit your changes or stash them before you can merge. 如果希望保留生产服务器上所做的改动,仅仅并入新配置项, 处理方法如下:123git stashgit pullgit stash pop 然后可以使用git diff -w +文件名 来确认代码自动合并的情况.反过来,如果希望用代码库中的文件完全覆盖本地工作版本. 方法如下:12git reset --hardgit pull 其中git reset是针对版本,如果想针对文件回退本地修改,使用:1git checkout HEAD file/to/restore git push时提示：更新被拒绝，因为您当前分支的最新提交落后于其对应的远程分支在对本地库中的文件执行修改后，想git push推送到远程库中，结果在git push的时候提示出错：12345! [rejected] master -&gt; master (non-fast-forward)error: 无法推送一些引用到 &apos;git@github.com:GarfieldEr007/XXXX.git&apos;提示：更新被拒绝，因为您当前分支的最新提交落后于其对应的远程分支。提示：再次推送前，先与远程变更合并（如 &apos;git pull ...&apos;）。详见提示：&apos;git push --help&apos; 中的 &apos;Note about fast-forwards&apos; 小节。 解决方案：因为当前分支的最新提交落后于其对应的远程分支，所以我们先从远程库fetch到更新再和本地库合并，之后就可以git push操作了。123git remote add origin https://github.com/username/test.git $git fetch origin $git merge origin/master","tags":[{"name":"问题","slug":"问题","permalink":"www.hzzzy.top/tags/问题/"}]},{"title":"git常用命令","date":"2017-01-11T07:35:57.000Z","path":"2017/01/11/git-order/","text":"平时工作大多用的是SVN，偶尔用用git，仅在自己搞自己项目的时候。就这样常常忘了git的命令，因不常用，用时只有个大概的印象。下面一张git命令图，可以经常查用。","tags":[{"name":"问题","slug":"问题","permalink":"www.hzzzy.top/tags/问题/"}]},{"title":"开博之笔","date":"2017-01-08T01:27:07.000Z","path":"2017/01/08/first-blog/","text":"开通自己的博客，记录自己所学，记录自己走过的路，谈谈技术，谈谈杂念。怕忘记了，怕找不到了，怕弄掉了，遂收集整理，形成博客。吾生也有涯，而知也无涯。博学之，审问之，慎思之，明辨之，笃行之。","tags":[{"name":"心路","slug":"心路","permalink":"www.hzzzy.top/tags/心路/"}]}]